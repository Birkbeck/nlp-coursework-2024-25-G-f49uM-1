{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e66577",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Part One ‚Äî Syntax and Style \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a6494",
   "metadata": {},
   "source": [
    "In the first part of your coursework, your task is to explore the syntax and style of a set of 19th Century novels using the methods and tools that you learned in class. The texts you need for this part are in the novels subdirectory of the texts directory in the coursework Moodle template. The texts are in plain text files, and the filenames include the title, author, and year of publication, separated by hyphens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5280ae",
   "metadata": {},
   "source": [
    "The template code provided in PartOne.py includes function headers for some sub-parts of this ques- tion. The main method of your finished script should call each of these functions in order. To complete your coursework, complete these functions so that they perform the tasks specified in the questions below. You may (and in some cases should) define additional functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269732d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Re-assessment template 2025\n",
    "\n",
    "Note: The template functions here and the dataframe format for structuring your solution is a suggested but not mandatory approach. You can use a different approach if you like, as long as you clearly answer the questions and communicate your answers clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ed11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# import nltk.word_tokenize # incorrect -- raises error see: https://www.nltk.org/api/nltk.tokenize.html\n",
    "# import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from math import log2\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # for copy warnings in pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f579f61",
   "metadata": {},
   "source": [
    "## read_novels:\n",
    "Each file in the novels directory contains the text of a novel, and the name of the file is the title, author, and year of publication of the novel, separated by hyphens. Complete the python function read_texts to do the following: \n",
    "- i. create a pandas dataframe with the following columns: text, title, author, year \n",
    "- ii. sort the dataframe by the year column before returning it, resetting or ignoring the dataframe index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1eb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_c</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nCHAPTER 1\\n\\nThe family of Dashwood had long...</td>\n",
       "      <td>The family of Dashwood had long been settled i...</td>\n",
       "      <td>Sense_and_Sensibility</td>\n",
       "      <td>Austen</td>\n",
       "      <td>1811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_c  \\\n",
       "0  \\nCHAPTER 1\\n\\nThe family of Dashwood had long...   \n",
       "\n",
       "                                                text                  title  \\\n",
       "0  The family of Dashwood had long been settled i...  Sense_and_Sensibility   \n",
       "\n",
       "   author  year  \n",
       "0  Austen  1811  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def remove_chapter_headings(text):\n",
    "    #pattern = r'chapter\\s*(?:[ivxIIVX]+)'\n",
    "    # pattern = r'(?i)CHAPTER \\d+'  # adding this so that chapter breaks are not counted. could be refined for section breaks etc.\n",
    "    pattern = r'(?i)chapter\\s*(?:[ivxIIVX]+|\\d+)' #* updating for both digits and roman numerals\n",
    "\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "\n",
    "def read_novels(path=Path.cwd() / \"zips\" / \"p1-texts\" / \"novels\"):\n",
    "    \"\"\"Reads texts from a directory of .txt files and returns a DataFrame with the text, title,\n",
    "    author, and year\"\"\"\n",
    "    files = [file for file in os.listdir(path) if file[-4:] == '.txt']\n",
    "    df_data = []\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        ''' Create a dictionary, and add the file items to it, then use populated dictionary to create a dataframe '''\n",
    "        file_dict = {}\n",
    "        file_dict['text'] = read_file(os.path.join(path,files[i]))\n",
    "        title, author, year = files[i].split('-')\n",
    "        file_dict['title'] = title \n",
    "\n",
    "        file_dict['author'] = author\n",
    "        file_dict['year'] = year[:-4]\n",
    "        df_data.append(file_dict)\n",
    "    df = pd.DataFrame(df_data)\n",
    "    ''' NOTE: I have opted to retain the original text as a copy, and have applied some cleaning to the documents\n",
    "    Cleaned text is applied to column \"text\"\n",
    "    '''\n",
    "    df['text_c'] = df['text'].copy() # retain original copy\n",
    "    mask = df['title'] == 'The_Black_Moth'\n",
    "    ind = df[mask].index[0]\n",
    "    df.loc[ind, 'text'] = df.loc[ind, 'text'][585:].strip()\n",
    "    df['text'] = df['text'].apply(lambda x: remove_chapter_headings(x).strip()) # apply function to all items in text column\n",
    "    df = df[['text_c','text', 'title', 'author', 'year']] # reorder columns\n",
    "    df = df.sort_values('year') # return sorted by year (assumed default ascending)\n",
    "    df.reset_index(inplace=True, drop=True)     \n",
    "    return df     \n",
    "    \n",
    "df = read_novels()\n",
    "# df[0:1]['text'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8dda8",
   "metadata": {},
   "source": [
    "## nltk_ttr: \n",
    "This function should return a dictionary mapping the title of each novel to its type-token ratio. Tokenize the text using the NLTK library only. Do not include punctuation as tokens, and ignore case when counting types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d77ee3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sense_and_Sensibility': 0.05288568542519132,\n",
       " 'North_and_South': 0.05492518524876262,\n",
       " 'A_Tale_of_Two_Cities': 0.07075401657114008,\n",
       " 'Erewhon': 0.09140189916402208,\n",
       " 'The_American': 0.06373402148472664,\n",
       " 'Dorian_Gray': 0.08359599310916864,\n",
       " 'Tess_of_the_DUrbervilles': 0.07781506237653235,\n",
       " 'The_Golden_Bowl': 0.04748457874040203,\n",
       " 'The_Secret_Garden': 0.058174923913585794,\n",
       " 'Portrait_of_the_Artist': 0.10497357039884671,\n",
       " 'The_Black_Moth': 0.07844770693814845,\n",
       " 'Orlando': 0.11390939170272334,\n",
       " 'Blood_Meridian': 0.08570454932697223}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nltk_ttr(text):\n",
    "    \"\"\"Calculates the type-token ratio of a text. Text is tokenized using nltk.word_tokenize.\"\"\"\n",
    "    text_cleaned = [t.lower() for t in word_tokenize(text) if t.isalnum()] # I've assumed that we want the numbers, as well as text, as the question didn't state to exclude these.\n",
    "    vocab = set(text_cleaned) # dedupe words\n",
    "    ttr = len(vocab) / len(text_cleaned) # ratio words to the text\n",
    "    return ttr\n",
    "\n",
    "\n",
    "def get_ttrs(df):\n",
    "    \"\"\"helper function to add ttr to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = nltk_ttr(row[\"text\"])\n",
    "    return results\n",
    "get_ttrs(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223eb32d",
   "metadata": {},
   "source": [
    "## c) flesch_kincaid\n",
    "This function should return a dictionary mapping the title of each novel to the Flesch-Kincaid reading grade level score of the text.  \n",
    "Use the NLTK library for tokenization and the CMU pronouncing dictionary for estimating syllable counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b11b2",
   "metadata": {},
   "source": [
    "[Readibility](https://readabilityformulas.com/learn-how-to-use-the-flesch-kincaid-grade-level/)\n",
    "$$\n",
    "{\\displaystyle 0.39 * \\left({\\frac {\\text{total words}}{\\text{total sentences}}}\\right)\n",
    "    +11.8\\left({\\frac {\\text{total syllables}}{\\text{total words}}}\\right)-15.59 }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a51dde5",
   "metadata": {},
   "source": [
    "### `count_syl()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b097f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import cmudict\n",
    "def count_syl(word, d):\n",
    "    \"\"\"Counts the number of syllables in a word given a dictionary of syllables per word.\n",
    "    if the word is not in the dictionary, syllables are estimated by counting vowel clusters\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to count syllables for.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of syllables in the word.\n",
    "    \"\"\"\n",
    "    counts = 0\n",
    "    pattern = r'[aeiouy]+'\n",
    "\n",
    "    if word in d:\n",
    "        word_cmu = d[word][0]\n",
    "        counts += sum(char[-1].isdigit() for char in word_cmu)\n",
    "    elif word not in d:\n",
    "        vowel_words = re.findall(pattern, word.lower())\n",
    "        counts += len(vowel_words)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc007655",
   "metadata": {},
   "source": [
    "##### Test for `count_syl()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaacaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = df['text'][0]\n",
    "# pattern = r'chapter [0-9] +'\n",
    "# text = text.replace('\\n', ' ').strip()\n",
    "# text = re.sub(pattern, '', text.lower())\n",
    "# d = cmudict.dict()\n",
    "# sents=sent_tokenize(text.lower())\n",
    "# words=[t.lower() for t in word_tokenize(text) if t.isalpha()]\n",
    "\n",
    "# x = words[0:10] # sample of the words\n",
    "\n",
    "# ''' Manual text of the function vs local use of the cmu dictionary '''\n",
    "\n",
    "# temp=dict()\n",
    "# for i in x:\n",
    "#     counts=0\n",
    "#     word_cmu = d[i][0]\n",
    "#     counts += sum(char[-1].isdigit() for char in word_cmu)\n",
    "#     temp[i] = (counts, word_cmu)\n",
    "# print([t[0] for t in temp.values()] == [count_syl(i, d) for i in x])\n",
    "\n",
    "# ''' And a test with some words not in the cmu dictionary'''\n",
    "# y = ['aaaa', 'maaattaan']\n",
    "# z = x+y\n",
    "# print(list(zip(z,[count_syl(i, d) for i in z])))\n",
    "# print(sum([count_syl(word, d) for word in z]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e405d",
   "metadata": {},
   "source": [
    "### `fk_level()`, `get_fks()`, `add_fks_to_df()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4017d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk \n",
    "import re\n",
    "def fk_level(text, d):\n",
    "    \"\"\"Returns the Flesch-Kincaid Grade Level of a text (higher grade is more difficult).\n",
    "    Requires a dictionary of syllables per word.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        float: The Flesch-Kincaid Grade Level of the text. (higher grade is more difficult)\n",
    "    \"\"\"\n",
    "    text = text.replace('\\n', ' ').strip() #* And removing line breaks \"\\n\"\n",
    "    sents=sent_tokenize(text.lower())\n",
    "    words=[t.lower() for t in word_tokenize(text) if t.isalpha()]\n",
    "    total_syllables = sum([count_syl(word, d) for word in words])\n",
    "    fkl = 0.39 * (len(words) / len(sents)) + 11.8 * (total_syllables/ len(words))- 15.59\n",
    "    # print(total_syllables, len(words), len(sents), fkl)\n",
    "\n",
    "    return fkl\n",
    "\n",
    "\n",
    "def get_fks(df):\n",
    "    \"\"\"helper function to add fk scores to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    cmudict = nltk.corpus.cmudict.dict()\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = round(fk_level(row[\"text\"], cmudict), 4)\n",
    "    return results\n",
    "\n",
    "def add_fks_to_df(df):\n",
    "    ''' function to add scores to the dataframe, as referenced in get_fks() '''\n",
    "    fks = get_fks(df)\n",
    "    temp = pd.DataFrame({'title_f': fks.keys(), 'fks':fks.values()})\n",
    "    df = df.merge(temp, left_on='title', right_on='title_f')\n",
    "    return df.drop(columns='title_f')\n",
    "fks_dict = get_fks(df) # return dictionary mapping\n",
    "df = add_fks_to_df(df) # add to df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762e657",
   "metadata": {},
   "source": [
    "## d) flesch_kincaid\n",
    "When is the Flesch Kincaid score *not* a valid, robust or reliable estimator of text difficulty? Give two conditions. (Text answer, 200 words maximum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5402b",
   "metadata": {},
   "source": [
    "**Etymology**: Lingustic differences of a vocabulary are not represented by the FKS. For example, anachronistic terminology which is no longer part of the modern vernacular are mechanistically oversimplified (the syllable to word ration); FKS would be misleading when scoring of the complexity of a text. Example - Chaucer's middle English:\n",
    "‚ÄúWhan that Aprill with his shoures soote/The droghte of March hath perced to the roote‚Äù (1).\n",
    "does not represent modern vernacular, and yet the syllable count and text length would deem its readibility as $\\approx$ 6th-7th grade/ 11-13 years old.\n",
    "\n",
    "**Context**: comprehension gained from word order or the organisation of the content is not factored into FKS (2). Scoring based on frequencies and ratios omits reader comprehension, contextual awareness and cultural understanding. Those for whom English as a second language, FKS is not robust, being based on the structure of English, and structures are not constant across language roots.\n",
    "\n",
    "Example - Unreliable structure with verb past tense, while syntax is comparible for present tense; different structures added complexity for comprehension.\n",
    "\n",
    "|         | German                            | English                   |\n",
    "| ------- | --------------------------------- | ------------------------- |\n",
    "| Past    | Ich habe einen Apfel **gegessen** | I **ate** an apple        |\n",
    "| Present | Ich **esse** einen Apfel        | I **am eating** an apple. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b002a",
   "metadata": {},
   "source": [
    "References\n",
    "  \n",
    "\n",
    "1) Canterbury Tales: General Prologue ll. 1-2, [source]([https://chaucer.fas.harvard.edu/pages/general-prologue-0](https://chaucer.fas.harvard.edu/pages/general-prologue-0)) accessed: 2025-06-18\n",
    "2) Redish, Janice. (2000). Readability formulas have even more limitations than Klare discusses. ACM Journal of Computer Documentation. 24. 132-137, pg. 134. DOI: 10.1145/344599.344637."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f6997",
   "metadata": {},
   "source": [
    "## e) parse\n",
    "The goal of this function is to process the texts with spaCy‚Äôs tokenizer and parser, and store the processed texts. Your completed function should: \n",
    "- i. Use the spaCy nlp method to add a new column to the dataframe that contains parsed and tokenized Doc objects for each text. \n",
    "- ii. Serialise the resulting dataframe (i.e., write it out to disk) using the pickle format. \n",
    "- iii. Return the dataframe. \n",
    "- iv. Load the dataframe from the pickle file and use it for the remainder of this coursework part. Note: one or more of the texts may exceed the default maximum length for spaCy‚Äôs model. You will need to either increase this length or parse the text in sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad318b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "def parse(df, store_path=Path.cwd() / \"pickles\", out_name=\"parsed.pickle\"):\n",
    "    \"\"\"Parses the text of a DataFrame using spaCy, stores the parsed docs as a column and writes the resulting  DataFrame to a pickle file\"\"\"\n",
    "    docs = {}\n",
    "    for i in trange(len(df)): # ! Uses TQDM notebook. Else switch to below...\n",
    "    # for i in range(len(df)): # ^ Switch out from TQDM notebook. \n",
    "        docs[i] = nlp(df['text'][i]) # create spacy docs for each cell \"text\"\n",
    "        # print(f\"Document {i+1} / {len(df)} parsed.\") #? added to see process when parsing (takes a while)\n",
    "    df['docs'] = docs.values()\n",
    "    df.to_pickle(os.path.join(store_path,out_name)) #~ write to pickle\n",
    "    ''' The question requested the df is returned - I have commented out this return, to purely save, as we load in the next step from pickle '''\n",
    "    # return df #^ To check the returned df\n",
    "parse(df) #^ function call to parse and save df to pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a63e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_c</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>year</th>\n",
       "      <th>fks</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nCHAPTER 1\\n\\nThe family of Dashwood had long...</td>\n",
       "      <td>The family of Dashwood had long been settled i...</td>\n",
       "      <td>Sense_and_Sensibility</td>\n",
       "      <td>Austen</td>\n",
       "      <td>1811</td>\n",
       "      <td>10.8845</td>\n",
       "      <td>(The, family, of, Dashwood, had, long, been, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Wooed and married and a'.'\\n'Edith!' said Mar...</td>\n",
       "      <td>'Wooed and married and a'.'\\n'Edith!' said Mar...</td>\n",
       "      <td>North_and_South</td>\n",
       "      <td>Gaskell</td>\n",
       "      <td>1855</td>\n",
       "      <td>6.6544</td>\n",
       "      <td>(', Wooed, and, married, and, a, ', ., ', \\n, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_c  \\\n",
       "0  \\nCHAPTER 1\\n\\nThe family of Dashwood had long...   \n",
       "1  'Wooed and married and a'.'\\n'Edith!' said Mar...   \n",
       "\n",
       "                                                text                  title  \\\n",
       "0  The family of Dashwood had long been settled i...  Sense_and_Sensibility   \n",
       "1  'Wooed and married and a'.'\\n'Edith!' said Mar...        North_and_South   \n",
       "\n",
       "    author  year      fks                                               docs  \n",
       "0   Austen  1811  10.8845  (The, family, of, Dashwood, had, long, been, s...  \n",
       "1  Gaskell  1855   6.6544  (', Wooed, and, married, and, a, ', ., ', \\n, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_from_pickle(store_path=Path.cwd() / \"pickles\", out_name=\"parsed.pickle\"):\n",
    "    df = pd.read_pickle(os.path.join(store_path, out_name))\n",
    "    return df\n",
    "\n",
    "\n",
    "''' Load from pickle '''\n",
    "df = load_from_pickle() \n",
    "df[0:2]\n",
    "#? To check the returned type\n",
    "# type(df['docs'][0]) # ~spacy.tokens.doc.Doc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f494c",
   "metadata": {},
   "source": [
    "## f) Working with parses:\n",
    "the final lines of the code template contain three `for` loops. Write the functions needed to complete these loops so that they print: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946f82d",
   "metadata": {},
   "source": [
    "- i. The title of each novel and a list of the ten most common syntactic objects overall in the text. \n",
    "- ii. The title of each novel and a list of the ten most common syntactic subjects of the verb ‚Äòto hear‚Äô (in any tense) in the text, ordered by their frequency. \n",
    "- iii. The title of each novel and a list of the ten most common syntactic subjects of the verb ‚Äòto hear‚Äô (in any tense) in the text, ordered by their Pointwise Mutual Information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a30472",
   "metadata": {},
   "source": [
    "### Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc9946",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<span style=\"color: #ff0000\">Note: </span> Novel 10 (Dorian Grey) does not have 10 words which are the subject of lemma 'hear', it will print 9.\n",
    "<p> \n",
    "<ul style=\"list-style-type: '&#x1F4BB;';\">\n",
    "\n",
    "<li> In the following code, the vocabulary is calculated per document, and total words (vocab of the document) declared as `N`. </li>\n",
    "</ul>\n",
    "This follows the PMI definition by Church and Hanks:\n",
    "<ul style=\"list-style-type: '&#x1F4C4;';\">\n",
    "<li><i>(1)Ref: *Word Association Norms, Mutual Information and Lexicography*, Church, K. W., and Hanks, P., in Computational Linguistics Volume 16, Number 1, March 1990</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850eb46",
   "metadata": {},
   "source": [
    "#### üí¨ PMI definitions, per Church and Hanks (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead0f5d",
   "metadata": {},
   "source": [
    "$$PMI = log_2\\bigg(\\frac{\\textcolor{gold}{P(x,y)}}{\\textcolor{cyan}{P(x)} \\textcolor{pink}{P(y)}}\\bigg)$$\n",
    "\n",
    "**Independent Probabilities**\n",
    "- are calculated as counts - f(x) and f(y) \n",
    "- normalised to $\\textcolor{lime}{N}$\n",
    "- $\\textcolor{cyan}{P(x)} = \\frac{f(x)}{\\textcolor{lime}{N}}$ and\n",
    "- $\\textcolor{pink}{P(y)} = \\frac{f(y)}{\\textcolor{lime}{N}}$\n",
    "\n",
    "**Joint Probabilities**\n",
    "- are calculated as counts of x and y co-occuring in a window - $f(x,y)$\n",
    "- using Spacy dependencies limits this to syntactical spans - the sentences of a document\n",
    "- normalised to $\\textcolor{lime}{N}$\n",
    "- $\\textcolor{gold}{P(x,y)} = \\frac{f(x,y)}{\\textcolor{lime}{N}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949bb098",
   "metadata": {},
   "source": [
    "### f i-iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def find_objects(token):\n",
    "    ''' Function to find the objects of a token  '''\n",
    "    objects = set()\n",
    "    if 'obj' in token.dep_:\n",
    "        objects.add(token)\n",
    "    return objects\n",
    "\n",
    "\n",
    "def objects_count(doc, n=10):\n",
    "    ''' Assumed \"object\" as syntactical, rather than a word as Python object '''\n",
    "    object_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        objects = find_objects(token)\n",
    "        if objects:\n",
    "            object_text.extend([objects.text.lower() for objects in objects]) #? have made this lower, to not duplicate the types\n",
    "\n",
    "    object_counts = Counter(object_text)\n",
    "    return  object_counts.most_common()[:n]\n",
    "# objects_count(df['docs'][0]) #*to test function with a sample doc\n",
    "\n",
    "def adjective_counts(doc, n=10):\n",
    "    \"\"\"Extracts the most common adjectives in a parsed document. Returns a list of tuples.\"\"\"\n",
    "    pos_counts = Counter([token.text for token in doc if token.pos_ == 'ADJ'])\n",
    "    return pos_counts.most_common()[:n]\n",
    "\n",
    "''' Adjective Counts '''\n",
    "# doc = df['docs'][len(df)-1] #*to test function with a sample doc\n",
    "# df['docs'].apply(lambda x: adjective_counts(x)) # if you want to count adjectives for whole df\n",
    "\n",
    "''' Manual test '''\n",
    "# adj_counts= Counter([token.text for token in doc if token.pos_ == 'ADJ'])\n",
    "# adj_counts.most_common()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1afa9053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense_and_Sensibility\n",
      "[('it', 706), ('her', 689), ('him', 521), ('them', 404), ('me', 343), ('you', 331), ('which', 251), ('what', 210), ('time', 193), ('herself', 193)]\n",
      "North_and_South\n",
      "[('it', 878), ('her', 863), ('him', 779), ('me', 579), ('you', 443), ('which', 415), ('what', 391), ('them', 365), ('time', 291), ('margaret', 240)]\n",
      "A_Tale_of_Two_Cities\n",
      "[('him', 897), ('it', 861), ('you', 434), ('me', 426), ('them', 372), ('her', 354), ('which', 220), ('hand', 205), ('himself', 185), ('time', 174)]\n",
      "Erewhon\n",
      "[('which', 415), ('me', 372), ('it', 358), ('them', 305), ('him', 184), ('that', 121), ('time', 120), ('what', 107), ('us', 101), ('one', 100)]\n",
      "The_American\n",
      "[('it', 875), ('you', 725), ('him', 710), ('me', 656), ('her', 569), ('what', 324), ('that', 295), ('them', 276), ('newman', 240), ('which', 225)]\n",
      "Dorian_Gray\n",
      "[('him', 609), ('it', 475), ('me', 442), ('you', 358), ('that', 253), ('them', 186), ('life', 180), ('her', 178), ('what', 177), ('one', 97)]\n",
      "Tess_of_the_DUrbervilles\n",
      "[('her', 862), ('it', 624), ('him', 566), ('me', 452), ('you', 400), ('them', 369), ('which', 367), ('that', 220), ('time', 205), ('what', 197)]\n",
      "The_Golden_Bowl\n",
      "[('it', 1794), ('her', 1503), ('him', 1241), ('which', 751), ('what', 744), ('them', 675), ('me', 489), ('you', 468), ('that', 416), ('time', 336)]\n",
      "The_Secret_Garden\n",
      "[('it', 625), ('him', 450), ('her', 354), ('me', 208), ('them', 207), ('garden', 164), ('what', 158), ('you', 143), ('things', 130), ('that', 112)]\n",
      "Portrait_of_the_Artist\n",
      "[('him', 609), ('it', 306), ('them', 214), ('which', 169), ('you', 158), ('god', 112), ('me', 112), ('eyes', 111), ('what', 109), ('that', 107)]\n",
      "The_Black_Moth\n",
      "[('him', 733), ('you', 528), ('me', 490), ('it', 471), ('her', 421), ('that', 162), ('hand', 156), ('_', 138), ('what', 134), ('eyes', 123)]\n",
      "Orlando\n",
      "[('it', 316), ('her', 280), ('him', 256), ('them', 218), ('which', 189), ('that', 98), ('time', 94), ('life', 77), ('what', 77), ('one', 69)]\n",
      "Blood_Meridian\n",
      "[('them', 662), ('him', 654), ('it', 575), ('man', 159), ('head', 157), ('horses', 151), ('hand', 137), ('horse', 134), ('fire', 122), ('what', 121)]\n"
     ]
    }
   ],
   "source": [
    "'''- i. The title of each novel and a list of the ten most common syntactic objects overall in the text. '''\n",
    "for i, row in df.iterrows():\n",
    "    print(row['title'])\n",
    "    print(objects_count(row['docs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb8c2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def freq_counter_dict_words(doc):\n",
    "    ''' \n",
    "    Returns a counter dictionary object for the words in the document \n",
    "    Only considers alphabet-character tokens (alpha) rather than excluding (e.g., with is_punct)\n",
    "    Use: freq_counter_dict_words(doc)\n",
    "    '''\n",
    "    from collections import Counter\n",
    "\n",
    "    words = [token.text.lower()\n",
    "            for token in doc\n",
    "            if token.is_alpha]\n",
    "    word_freq = Counter(words)\n",
    "    return word_freq\n",
    "# freq_counter_dict_words(df['docs'][0]) #*to test function with a sample doc\n",
    "\n",
    "\n",
    "def get_verbs(doc, verb='hear'):\n",
    "    ''' Function to get the count of a verb. Retains original form '''\n",
    "    verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.lemma_==verb:\n",
    "\n",
    "            verbs.append(token.text.lower())\n",
    "    verb_counts =Counter(verbs)\n",
    "    return verb_counts #~ retains the original verb form, returns their frequency\n",
    "\n",
    "def sum_verbs(doc, verb='hear'):\n",
    "    ''' Get the total sum of verbs matched with given lemma '''\n",
    "    verb_counts = get_verbs(doc)\n",
    "    return (verb, sum(verb_counts.values()))\n",
    "\n",
    "def find_subjects(token, match_verb):\n",
    "    ''' Function to find the subjects of a token - both up and down the parse tree '''\n",
    "    subjects = []\n",
    "    ''' Token's children '''\n",
    "\n",
    "    for child in token.children:\n",
    "        if 'subj' in child.dep_:\n",
    "            subjects.append((child, token))\n",
    "                \n",
    "    return subjects\n",
    "\n",
    "\n",
    "def subjects_by_verb_count(doc, verb='hear', n=10):\n",
    "    \"\"\"Extracts the most common subjects of a given verb in a parsed document. Returns a list.\"\"\"\n",
    "    ''' Added a parameter to remove multiple counts of 1 in the most frequent.\n",
    "    > Setting to n=10 sliced the list by order, which didn't have much meaning in the returned values\n",
    "    '''\n",
    "    subject_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\" and token.lemma_ == verb:\n",
    "            subjects = find_subjects(token, verb)\n",
    "            if subjects:\n",
    "                for subject in subjects:\n",
    "                    if not subject[0].is_punct: #~ adding this to ignore punctuation\n",
    "                        subject_text.extend([(subject[0].text.lower(), subject[1]) for subject in subjects])\n",
    "\n",
    "    return  Counter([s for s, v in subject_text]).most_common()[:n]\n",
    "    # return  Counter([s for s, v in subjects_by_verb_count(doc, verb='hear')]).most_common()[:n]\n",
    "# doc=df['docs'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c8537ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense_and_Sensibility [('i', 28), ('you', 19), ('she', 14), ('elinor', 6), ('he', 6), ('they', 5), ('me', 4), ('jennings', 3), ('them', 2), ('both', 2)]\n",
      "North_and_South [('she', 60), ('i', 42), ('he', 23), ('you', 16), ('they', 13), ('margaret', 10), ('me', 5), ('we', 5), ('who', 4), ('thornton', 3)]\n",
      "A_Tale_of_Two_Cities [('i', 21), ('he', 19), ('you', 12), ('she', 10), ('they', 5), ('stranger', 2), ('clink', 2), ('monseigneur', 2), ('me', 2), ('him', 2)]\n",
      "Erewhon [('i', 39), ('he', 4), ('they', 3), ('she', 2), ('formulae', 2), ('we', 1), ('who', 1), ('destruction', 1), ('machines', 1), ('one', 1)]\n",
      "The_American [('he', 18), ('i', 13), ('you', 10), ('she', 5), ('newman', 4), ('they', 2), ('we', 2), ('who', 1), ('me', 1), ('one', 1)]\n",
      "Dorian_Gray [('i', 24), ('he', 16), ('one', 3), ('you', 3), ('lovers', 1), ('hast', 1), ('who', 1), ('jars', 1), ('dorian', 1)]\n",
      "Tess_of_the_DUrbervilles [('she', 39), ('i', 20), ('they', 12), ('he', 8), ('you', 8), ('who', 6), ('tess', 4), ('clare', 4), ('lady', 2), ('room', 2)]\n",
      "The_Golden_Bowl [('she', 16), ('you', 7), ('he', 6), ('which', 3), ('i', 3), ('maggie', 2), ('him', 2), ('man', 1), ('they', 1), ('it', 1)]\n",
      "The_Secret_Garden [('i', 28), ('she', 25), ('he', 16), ('you', 8), ('we', 6), ('mary', 3), ('they', 3), ('lennox', 2), ('colin', 2), ('one', 2)]\n",
      "Portrait_of_the_Artist [('he', 63), ('you', 12), ('i', 8), ('stephen', 5), ('voice', 3), ('they', 2), ('burst', 2), ('listener', 2), ('that', 2), ('who', 2)]\n",
      "The_Black_Moth [('i', 37), ('he', 5), ('you', 5), ('we', 5), ('richard', 2), ('madam', 2), ('she', 2), ('nothing', 1), ('street', 1), ('it', 1)]\n",
      "Orlando [('she', 22), ('he', 8), ('orlando', 5), ('they', 3), ('one', 3), ('i', 2), ('which', 2), ('children', 2), ('hoof', 1), ('it', 1)]\n",
      "Blood_Meridian [('he', 25), ('they', 21), ('i', 6), ('you', 5), ('who', 3), ('she', 2), ('all', 2), ('we', 2), ('man', 2), ('nobody', 1)]\n"
     ]
    }
   ],
   "source": [
    "''' - ii. The title of each novel and a list of the ten most common syntactic subjects of the verb ‚Äòto hear‚Äô (in any tense) in the text, ordered by their frequency. '''\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(row['title'], subjects_by_verb_count(row['docs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18c1c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_log2(x):\n",
    "    ''' Base2 logarithm '''\n",
    "    return log2(x)\n",
    "\n",
    "def get_verb_freqs(doc):\n",
    "    ''' Treating the verb frequencies together '''\n",
    "    verb_counts = get_verbs(doc)\n",
    "    return sum(verb_counts.values()) # consider the verbs as a single verb (as we are treating 'hear' as tense-insensitive)\n",
    "\n",
    "def calculate_probs(x, N):\n",
    "    ''' A word or verb, divided by total vocab count, for example '''\n",
    "    return x / N\n",
    "\n",
    "def make_freq_p_df(doc):\n",
    "    ''' Create dataframe of top ten co-occurring word-verb pairs\n",
    "    This treats all verbs as a tense-insensitive lemmatised version\n",
    "    - i.e., considers all verbs as the target verb - 'hear'\n",
    "    '''\n",
    "    all_word_freq = freq_counter_dict_words(doc)\n",
    "    N = sum(all_word_freq.values()) # unique word total (N)\n",
    "    top_ten = subjects_by_verb_count(doc, verb='hear')\n",
    "    df_c = pd.DataFrame(data=top_ten, columns=['word', 'f_word_verb'])\n",
    "    df_c['f_word'] = df_c['word'].map(all_word_freq)\n",
    "    df_c['p_word_verb'] = df_c['f_word_verb'].apply(lambda x: calculate_probs(x,N))\n",
    "    df_c['p_verb'] = calculate_probs(get_verb_freqs(doc),N) #? consider the verbs as a single verb (as we are treating 'hear' as tense-insensitive)\n",
    "    df_c['p_word'] = df_c['f_word'].apply(lambda x: calculate_probs(x, N))\n",
    "    return df_c[['word', 'f_word', 'f_word_verb', 'p_word','p_verb', 'p_word_verb']] # ~ reorder\n",
    "\n",
    "\n",
    "def calculate_pmi(df):\n",
    "    ''' The co-occurrence probabilities, scaled to independent probability product\n",
    "    Then log_2 of this normalised value\n",
    "    See markdown note on reference and approach\n",
    "    '''\n",
    "    df['P(w)P(v)'] = df['p_word']*df['p_verb'] #~ product of independent\n",
    "    df['fracts'] = df['p_word_verb'] / df['P(w)P(v)'] # ~ joint probabilities scaled to product of independents \n",
    "    df['pmi'] = df['fracts'].apply(lambda x: apply_log2(x)) #~ apply log2 to result for pmi\n",
    "    return df\n",
    "\n",
    "def select_sort_df(df):\n",
    "    ''' Sort by descending, reset index '''\n",
    "    df = df[['word', 'pmi']].sort_values('pmi', ascending=False)\n",
    "    return df.reset_index( drop=True)\n",
    "\n",
    "\n",
    "def subjects_by_verb_pmi(doc, target_verb):\n",
    "    \"\"\"Extracts the most common subjects of a given verb in a parsed document. Returns a list.\"\"\"\n",
    "    df_pmi = make_freq_p_df(doc)\n",
    "    df_pmi = calculate_pmi(df_pmi)\n",
    "    return [select_sort_df(df_pmi)] # ~ returns a list ;)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28a88798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense_and_Sensibility\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>both</td>\n",
       "      <td>you</td>\n",
       "      <td>i</td>\n",
       "      <td>jennings</td>\n",
       "      <td>they</td>\n",
       "      <td>me</td>\n",
       "      <td>she</td>\n",
       "      <td>elinor</td>\n",
       "      <td>he</td>\n",
       "      <td>them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>3.899939</td>\n",
       "      <td>3.4935</td>\n",
       "      <td>3.309079</td>\n",
       "      <td>3.153695</td>\n",
       "      <td>2.758211</td>\n",
       "      <td>2.697631</td>\n",
       "      <td>2.620744</td>\n",
       "      <td>2.614669</td>\n",
       "      <td>1.928663</td>\n",
       "      <td>1.581117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North_and_South\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>she</td>\n",
       "      <td>they</td>\n",
       "      <td>we</td>\n",
       "      <td>i</td>\n",
       "      <td>who</td>\n",
       "      <td>he</td>\n",
       "      <td>you</td>\n",
       "      <td>margaret</td>\n",
       "      <td>me</td>\n",
       "      <td>thornton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>3.785776</td>\n",
       "      <td>3.399851</td>\n",
       "      <td>2.938283</td>\n",
       "      <td>2.88932</td>\n",
       "      <td>2.706553</td>\n",
       "      <td>2.527199</td>\n",
       "      <td>2.48084</td>\n",
       "      <td>2.199301</td>\n",
       "      <td>2.057408</td>\n",
       "      <td>1.74158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_Tale_of_Two_Cities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>clink</td>\n",
       "      <td>stranger</td>\n",
       "      <td>she</td>\n",
       "      <td>monseigneur</td>\n",
       "      <td>i</td>\n",
       "      <td>he</td>\n",
       "      <td>they</td>\n",
       "      <td>you</td>\n",
       "      <td>me</td>\n",
       "      <td>him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>11.160835</td>\n",
       "      <td>7.460396</td>\n",
       "      <td>4.634141</td>\n",
       "      <td>4.419368</td>\n",
       "      <td>3.612105</td>\n",
       "      <td>3.551559</td>\n",
       "      <td>3.335559</td>\n",
       "      <td>3.273107</td>\n",
       "      <td>2.116441</td>\n",
       "      <td>1.234539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erewhon\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>formulae</td>\n",
       "      <td>destruction</td>\n",
       "      <td>i</td>\n",
       "      <td>she</td>\n",
       "      <td>machines</td>\n",
       "      <td>he</td>\n",
       "      <td>who</td>\n",
       "      <td>they</td>\n",
       "      <td>one</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>10.15276</td>\n",
       "      <td>7.567798</td>\n",
       "      <td>4.602901</td>\n",
       "      <td>3.848979</td>\n",
       "      <td>3.81291</td>\n",
       "      <td>2.82858</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>2.047725</td>\n",
       "      <td>1.617485</td>\n",
       "      <td>1.523404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_American\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>we</td>\n",
       "      <td>he</td>\n",
       "      <td>they</td>\n",
       "      <td>who</td>\n",
       "      <td>i</td>\n",
       "      <td>you</td>\n",
       "      <td>she</td>\n",
       "      <td>one</td>\n",
       "      <td>newman</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>3.381236</td>\n",
       "      <td>3.285816</td>\n",
       "      <td>2.749944</td>\n",
       "      <td>2.61037</td>\n",
       "      <td>2.508956</td>\n",
       "      <td>2.424763</td>\n",
       "      <td>2.262193</td>\n",
       "      <td>2.179164</td>\n",
       "      <td>1.999043</td>\n",
       "      <td>0.804705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dorian_Gray\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>hast</td>\n",
       "      <td>jars</td>\n",
       "      <td>lovers</td>\n",
       "      <td>i</td>\n",
       "      <td>he</td>\n",
       "      <td>one</td>\n",
       "      <td>who</td>\n",
       "      <td>dorian</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>10.348039</td>\n",
       "      <td>9.348039</td>\n",
       "      <td>8.763076</td>\n",
       "      <td>4.207635</td>\n",
       "      <td>3.759324</td>\n",
       "      <td>3.164817</td>\n",
       "      <td>2.540684</td>\n",
       "      <td>1.654552</td>\n",
       "      <td>1.438146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tess_of_the_DUrbervilles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>lady</td>\n",
       "      <td>room</td>\n",
       "      <td>she</td>\n",
       "      <td>who</td>\n",
       "      <td>they</td>\n",
       "      <td>clare</td>\n",
       "      <td>i</td>\n",
       "      <td>you</td>\n",
       "      <td>tess</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>4.904466</td>\n",
       "      <td>4.350868</td>\n",
       "      <td>4.021884</td>\n",
       "      <td>3.927925</td>\n",
       "      <td>3.672135</td>\n",
       "      <td>3.445034</td>\n",
       "      <td>3.297947</td>\n",
       "      <td>2.2737</td>\n",
       "      <td>2.056146</td>\n",
       "      <td>2.022612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_Golden_Bowl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>man</td>\n",
       "      <td>she</td>\n",
       "      <td>you</td>\n",
       "      <td>maggie</td>\n",
       "      <td>which</td>\n",
       "      <td>he</td>\n",
       "      <td>him</td>\n",
       "      <td>i</td>\n",
       "      <td>they</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>4.547313</td>\n",
       "      <td>3.57273</td>\n",
       "      <td>3.416992</td>\n",
       "      <td>3.198065</td>\n",
       "      <td>3.194599</td>\n",
       "      <td>2.756214</td>\n",
       "      <td>2.183565</td>\n",
       "      <td>2.093034</td>\n",
       "      <td>1.594486</td>\n",
       "      <td>-0.464572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_Secret_Garden\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>lennox</td>\n",
       "      <td>we</td>\n",
       "      <td>i</td>\n",
       "      <td>she</td>\n",
       "      <td>you</td>\n",
       "      <td>he</td>\n",
       "      <td>they</td>\n",
       "      <td>colin</td>\n",
       "      <td>one</td>\n",
       "      <td>mary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>6.012931</td>\n",
       "      <td>4.852467</td>\n",
       "      <td>3.438107</td>\n",
       "      <td>2.91057</td>\n",
       "      <td>2.456538</td>\n",
       "      <td>2.17781</td>\n",
       "      <td>1.772467</td>\n",
       "      <td>1.729707</td>\n",
       "      <td>1.442183</td>\n",
       "      <td>1.246565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portrait_of_the_Artist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>listener</td>\n",
       "      <td>burst</td>\n",
       "      <td>he</td>\n",
       "      <td>voice</td>\n",
       "      <td>you</td>\n",
       "      <td>i</td>\n",
       "      <td>who</td>\n",
       "      <td>stephen</td>\n",
       "      <td>they</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>9.137738</td>\n",
       "      <td>7.330383</td>\n",
       "      <td>4.240036</td>\n",
       "      <td>4.008455</td>\n",
       "      <td>3.228845</td>\n",
       "      <td>2.91615</td>\n",
       "      <td>2.81581</td>\n",
       "      <td>2.701442</td>\n",
       "      <td>1.541548</td>\n",
       "      <td>0.184996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_Black_Moth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>we</td>\n",
       "      <td>street</td>\n",
       "      <td>madam</td>\n",
       "      <td>i</td>\n",
       "      <td>nothing</td>\n",
       "      <td>richard</td>\n",
       "      <td>you</td>\n",
       "      <td>he</td>\n",
       "      <td>she</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>5.048453</td>\n",
       "      <td>5.017202</td>\n",
       "      <td>4.557771</td>\n",
       "      <td>3.682519</td>\n",
       "      <td>3.634733</td>\n",
       "      <td>2.780163</td>\n",
       "      <td>1.215009</td>\n",
       "      <td>1.174223</td>\n",
       "      <td>0.678625</td>\n",
       "      <td>-0.092815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orlando\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>hoof</td>\n",
       "      <td>children</td>\n",
       "      <td>she</td>\n",
       "      <td>orlando</td>\n",
       "      <td>i</td>\n",
       "      <td>he</td>\n",
       "      <td>one</td>\n",
       "      <td>they</td>\n",
       "      <td>which</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>9.905097</td>\n",
       "      <td>7.905097</td>\n",
       "      <td>4.011382</td>\n",
       "      <td>3.435862</td>\n",
       "      <td>3.283045</td>\n",
       "      <td>3.222102</td>\n",
       "      <td>2.943165</td>\n",
       "      <td>2.912631</td>\n",
       "      <td>1.922103</td>\n",
       "      <td>-0.031541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blood_Meridian\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>nobody</td>\n",
       "      <td>who</td>\n",
       "      <td>she</td>\n",
       "      <td>they</td>\n",
       "      <td>i</td>\n",
       "      <td>he</td>\n",
       "      <td>we</td>\n",
       "      <td>you</td>\n",
       "      <td>all</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmi</th>\n",
       "      <td>7.104102</td>\n",
       "      <td>4.689065</td>\n",
       "      <td>4.51914</td>\n",
       "      <td>4.104961</td>\n",
       "      <td>4.059708</td>\n",
       "      <td>3.686182</td>\n",
       "      <td>3.561844</td>\n",
       "      <td>3.543387</td>\n",
       "      <td>2.882999</td>\n",
       "      <td>2.368039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# *iii. The title of each novel and a list of the ten most common syntactic subjects of the verb ‚Äòto hear‚Äô (in any tense) in the text, ordered by their Pointwise Mutual Information.'''\n",
    "for i, row in df.iterrows():\n",
    "    # Note: Dorian_Gray has 9 verb-word pairs where word is subject '''\n",
    "    title = row['title']\n",
    "    print(title)\n",
    "    # print(subjects_by_verb_pmi(row[\"docs\"], \"hear\"))\n",
    "    display(HTML(subjects_by_verb_pmi(row['docs'], 'hear')[0].T.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba90f4",
   "metadata": {},
   "source": [
    "## g) Ten marks are allocated for your github commit history. \n",
    "You should make regular, atomic commits with concise but informative commit messages. See the section titled Submission (both questions) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "''' Adding pprint for better dictionary output '''\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" uncomment the following lines to run the functions once you have completed them\"\"\"\n",
    "    # path = Path.cwd() / \"p1-texts\" / \"novels\"\n",
    "    # print(path)\n",
    "    #df = read_novels(path) # this line will fail until you have completed the read_novels function above.\n",
    "    #print(df.head())\n",
    "    #nltk.download(\"cmudict\")\n",
    "    #parse(df)\n",
    "    # print(df.head()[0:2])\n",
    "    # pprint(get_ttrs(df))\n",
    "    # pprint(get_fks(df))\n",
    "    # df = pd.read_pickle(Path.cwd() / \"pickles\" /\"name.pickle\")\n",
    "\n",
    "    # for i, row in df.iterrows():\n",
    "    #     print(row['title'], ': ',adjective_counts(row['docs']),'\\n') #? this is using the dataframe, the function and docstring calls for a `doc` object.\n",
    "   \n",
    "    # for i, row in df.iterrows():\n",
    "    #     print(row[\"title\"])\n",
    "    #     print(subjects_by_verb_count(row[\"docs\"], \"hear\"))\n",
    "    #     print(\"\\n\")\n",
    "\n",
    "    # for i, row in df.iterrows():\n",
    "    #     print(row[\"title\"])\n",
    "    #     print(subjects_by_verb_pmi(row[\"docs\"], \"hear\"))\n",
    "    #     print(\"\\n\")\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e32ce",
   "metadata": {},
   "source": [
    "# Part Two ‚Äî Feature Extraction and Classification \n",
    "In the second part of the coursework, your task is to train and test machine learning classifiers on a dataset of political speeches. The objective is to learn to predict the political party from the text of the speech. The texts you need for this part are in the speeches sub-directory of the texts directory of the coursework Moodle template. For this part, you can structure your python functions in any way that you like, but pay attention to exactly what information (if any) you are asked to print out in each part. Your final script should print out the answers to each part where required, and nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0751f3",
   "metadata": {},
   "source": [
    "## a) Read the hansard40000.csv dataset in the texts directory into a dataframe. \n",
    "Sub- set and rename the dataframe as follows: \n",
    "- i. rename the ‚ÄòLabour (Co-op)‚Äô value in ‚Äòparty‚Äô column to ‚ÄòLabour‚Äô, and then: \n",
    "- ii. remove any rows where the value of the ‚Äòparty‚Äô column is not one of the four most common party names, and remove the ‚ÄòSpeaker‚Äô value. \n",
    "- iii. remove any rows where the value in the ‚Äòspeech_class‚Äô column is not ‚ÄòSpeech‚Äô. \n",
    "- iv. remove any rows where the text in the ‚Äòspeech‚Äô column is less than 1000 characters long\n",
    "- Print the dimensions of the resulting dataframe using the shape method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03c8fc",
   "metadata": {},
   "source": [
    "Assuming we ignore blanks (`NaN`)\n",
    "\n",
    "`df['speech_class'].value_counts(dropna=False)`\n",
    "```\n",
    "party\n",
    "Conservative                        25079\n",
    "Labour                               8038\n",
    "Scottish National Party              2303\n",
    "NaN                                  1647\n",
    ".....\n",
    "\n",
    "`df['speech_class'].value_counts()`\n",
    "The top 4n less \"Speaker\"\n",
    "party\n",
    "Conservative                        25079\n",
    "Labour                               8038\n",
    "Scottish National Party              2303\n",
    "Liberal Democrat                      803\n",
    ".....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b39de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a990e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_df(path=Path.cwd() / \"zips\" / \"p2-texts\"):\n",
    "    df = pd.read_csv(os.path.join(path,os.listdir(path)[0]))\n",
    "\n",
    "    #^ - i. rename the ‚ÄòLabour (Co-op)‚Äô value in ‚Äòparty‚Äô column to ‚ÄòLabour‚Äô, and then: \n",
    "    df['party'].replace(\"Labour (Co-op)\", \"Labour\", inplace=True)\n",
    "\n",
    "\n",
    "    #^ - ii. remove any rows where the value of the ‚Äòparty‚Äô column is not one of the four most common party names, and remove the ‚ÄòSpeaker‚Äô value. \n",
    "    values = ['Conservative', 'Labour', 'Scottish National Party', 'Liberal Democrat',]\n",
    "    df = df[df['party'].isin(values)]\n",
    "\n",
    "    #^ - iii. remove any rows where the value in the ‚Äòspeech_class‚Äô column is not ‚ÄòSpeech‚Äô. \n",
    "    df = df[df['speech_class'] == 'Speech']\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True) #? NB: needs reseting, otherwise the next step doesn't work\n",
    "\n",
    "\n",
    "    #^ - iv. remove any rows where the text in the ‚Äòspeech‚Äô column is less than 1000 characters long\n",
    "    # indices= []\n",
    "    ind= []\n",
    "    for i, s in enumerate(df['speech'].values):\n",
    "        # print(len(s))\n",
    "        if len(s) < 1000:\n",
    "            # indices.append((i, len(s)))\n",
    "            ind.append(i)\n",
    "\n",
    "    df = df[~df.index.isin(ind)]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    #^ - Print the dimensions of the resulting dataframe using the shape method.\n",
    "    # len(df) == 36223-28139\n",
    "    print(df.shape)\n",
    "    return df #? Although this might not be wanted? \"Function to print and nothing else?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49eb3912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8084, 8)\n"
     ]
    }
   ],
   "source": [
    "df = csv_to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07271048",
   "metadata": {},
   "source": [
    "## b) Vectorise the speeches using TfidfVectorizer from scikit-learn. \n",
    "Use the default parameters, except for omitting English stopwords and setting max_features to 3000. Split the data into a train and test set, using stratified sampling, with a random seed of 26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d0c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score,  classification_report\n",
    "from sklearn import svm\n",
    "from pathlib import Path\n",
    "path = Path(Path.cwd() / \"pickles\" / \"speeches.pkl\")\n",
    "df = pd.read_pickle(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Stratify \"If not None, data is split in a stratified fashion, using this as the class labels.\"\n",
    "source: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "'''\n",
    "\n",
    "def establish_tfidf(texts, **kwargs):\n",
    "    ''' Pass texts without labels and kwargs for the TfidfVectorizer '''\n",
    "    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n",
    "    return tfidf_vectorizer.fit_transform(texts) \n",
    "\n",
    "def establish_split(vectorised, **kwargs):\n",
    "    ''' Pass vectorised dataset and kwargs for sample split '''\n",
    "    return train_test_split(vectorised, y, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9789e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = df.speech\n",
    "y = df.party\n",
    "\n",
    "vec_kw={\n",
    "    'stop_words':\"english\",\n",
    "    'max_features': 3000,\n",
    "    }\n",
    "\n",
    "tts_kw = {\n",
    "    'random_state':26,\n",
    "    'shuffle': True,\n",
    "    'stratify':y\n",
    "}\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = establish_split(establish_tfidf(x, **vec_kw), **tts_kw) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c2f73",
   "metadata": {},
   "source": [
    "## c) Random forest and SVM\n",
    "Train **RandomForest** (with n_estimators=300) and **SVM** with linear kernel classifiers on the training set, and print \n",
    "\n",
    "- the scikit-learn macro-average f1 score and\n",
    "- classification report for each classifier on the test set.\n",
    "\n",
    "The label that you are trying to predict is the ‚Äòparty‚Äô value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25deca",
   "metadata": {},
   "source": [
    "$$F1 = \\frac{TP}{TP+\\frac{1}{2}(FP+FN)}$$\n",
    "- best value at 1 \n",
    "- worst score at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test, average,**kwargs):\n",
    "    rfc = RandomForestClassifier(**kwargs)\n",
    "    rfc_fitted = rfc.fit(X_train, y_train)\n",
    "\n",
    "    print(f1_score(y_test, rfc_fitted.predict(X_test), average=average))\n",
    "    # params (y_true, y_predicted)\n",
    "    print(classification_report(y_test, rfc.predict(X_test)))\n",
    "\n",
    "def svm_cl(X_train, y_train, X_test, y_test,average, **kwargs):\n",
    "    linear_svc = svm.SVC(**kwargs)\n",
    "    linear_svc_fitted = linear_svc.fit(X_train, y_train)\n",
    "    linear_svc_fitted.predict(X_test)\n",
    "    \n",
    "    print(f1_score(y_test, linear_svc.predict(X_test), average=average))\n",
    "    print(classification_report(y_test, linear_svc_fitted.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f7103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44849276102645497\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.73      0.97      0.83      1205\n",
      "                 Labour       0.74      0.45      0.56       579\n",
      "       Liberal Democrat       0.00      0.00      0.00        67\n",
      "Scottish National Party       0.88      0.26      0.40       170\n",
      "\n",
      "               accuracy                           0.73      2021\n",
      "              macro avg       0.59      0.42      0.45      2021\n",
      "           weighted avg       0.72      0.73      0.69      2021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfkw = {\n",
    "    'random_state':26,\n",
    "    'n_estimators' : 300, \n",
    "}\n",
    "\n",
    "random_forest(X_train, y_train, X_test, y_test, average='macro', **rfkw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ee15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5846137591595653\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.82      0.92      0.87      1205\n",
      "                 Labour       0.72      0.68      0.70       579\n",
      "       Liberal Democrat       0.83      0.07      0.14        67\n",
      "Scottish National Party       0.78      0.53      0.63       170\n",
      "\n",
      "               accuracy                           0.79      2021\n",
      "              macro avg       0.79      0.55      0.58      2021\n",
      "           weighted avg       0.79      0.79      0.78      2021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svmkw = {\n",
    "    'kernel': 'linear',\n",
    "    'random_state':26,\n",
    "}\n",
    "\n",
    "svm_cl(X_train, y_train, X_test, y_test, average='macro',**svmkw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885cf15",
   "metadata": {},
   "source": [
    "## d) N-Grams\n",
    "Adjust the parameters of the Tfidfvectorizer so that unigrams, bi-grams and tri-grams will be considered as features, limiting the total number of features to 3000. Print the classification report as in 2(c) again using these parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79fe8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' establish new vector with ngrams (unigram, bigram and trigram)'''\n",
    "vec_kw={\n",
    "    'stop_words':\"english\",\n",
    "    'max_features': 3000,\n",
    "    'ngram_range' : (1,3)\n",
    "    }\n",
    "\n",
    "tts_kw = {\n",
    "    'random_state':26,\n",
    "    'shuffle': True,\n",
    "    'stratify':y\n",
    "}\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = establish_split(establish_tfidf(x, **vec_kw), **tts_kw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad01b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4775447410650979\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.73      0.97      0.84      1205\n",
      "                 Labour       0.77      0.47      0.58       579\n",
      "       Liberal Democrat       0.00      0.00      0.00        67\n",
      "Scottish National Party       0.86      0.35      0.49       170\n",
      "\n",
      "               accuracy                           0.74      2021\n",
      "              macro avg       0.59      0.45      0.48      2021\n",
      "           weighted avg       0.73      0.74      0.71      2021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_kw = {\n",
    "    'random_state':26,\n",
    "    'n_estimators' : 300, \n",
    "}\n",
    "\n",
    "random_forest(X_train2, y_train2, X_test2, y_test2, average='macro', **rf_kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7dbbb",
   "metadata": {},
   "source": [
    "Previous: 0.44849276102645497  \n",
    "With ngrams: 0.4775447410650979"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edc1eea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5741880652063533\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.83      0.92      0.87      1205\n",
      "                 Labour       0.73      0.72      0.73       579\n",
      "       Liberal Democrat       1.00      0.03      0.06        67\n",
      "Scottish National Party       0.78      0.55      0.64       170\n",
      "\n",
      "               accuracy                           0.80      2021\n",
      "              macro avg       0.83      0.55      0.57      2021\n",
      "           weighted avg       0.80      0.80      0.78      2021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_kw = {\n",
    "    'kernel': 'linear',\n",
    "    'random_state':26,\n",
    "}\n",
    "svm_cl(X_train2, y_train2, X_test2, y_test2, average='macro',**svm_kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcc539",
   "metadata": {},
   "source": [
    "Previous: 0.5846137591595653  \n",
    "With ngrams: 0.5741880652063533"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c9d2c",
   "metadata": {},
   "source": [
    "## e) Implement a new custom tokenizer and pass it to the tokenizer argument of Tfidfvectorizer. \n",
    "You can use this function in any way you like to try to achieve the best classification performance while keeping the number of features to no more than 3000, and using the same three classifiers as above. Print the clas- sification report for the best performing classifier using your tokenizer. Marks will be awarded both for a high overall classification performance, and a good trade-off between classification performance and efficiency (i.e., using fewer parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a322d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# path = os.path.join('pickles', 'speeches.pkl')\n",
    "# df.to_pickle(path) # saving to pickle it doesn't require parsing every load (~15m)\n",
    "# df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28702d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#* RandomForest (with n_estimators=300) \n",
    "#* SVM Linear\n",
    "\n",
    "#* Adjust the parameters of the Tfidfvectorizer\n",
    "# * so that unigrams, bi-grams and tri-grams \n",
    "# *will be considered as features, limiting the\n",
    "# * total number of features to 3000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "493c56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "def list_to_str(lst):\n",
    "    ''' take a list of words or Spacy tokens and return a single joined string ''' \n",
    "    if lst and isinstance(lst[0], spacy.tokens.span.Span):\n",
    "        return ' '.join([x.text for x in lst])\n",
    "    elif lst and isinstance(lst[0], str):\n",
    "        return ' '.join([x for x in lst])\n",
    "    else:\n",
    "        print('provide list of spaCy spans or string')\n",
    "\n",
    "def gensim_simple(doc, as_list=False):\n",
    "    if isinstance(doc[0], spacy.tokens.span.Span):\n",
    "        doc = doc.text    \n",
    "    else:\n",
    "        doc=doc\n",
    "        if as_list:\n",
    "            return list_to_str(simple_preprocess(doc))\n",
    "        else:\n",
    "            return simple_preprocess(doc)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df['speech_pp']= df['docs'].apply(lambda x: gensim_simple(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e82e04",
   "metadata": {},
   "source": [
    "## f) Explain your tokenizer function and discuss its performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf42e72",
   "metadata": {},
   "source": [
    "## g) Githistory\n",
    "Ten marks are allocated for your github commit history. You should make regular, atomic commits with concise but informative commit messages. See the section below titled Submission (both questions) for more details. Part Two total marks: 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86baa4d6",
   "metadata": {},
   "source": [
    "# Git requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c7cfc",
   "metadata": {},
   "source": [
    "Submit using github classroom and confirm on Moodle The code template for this coursework part is made available to you as a Git repository on GitHub, via an invitation link for GitHub Classroom. \n",
    "\n",
    "- 1. First you follow the invitation link for the coursework that is available on the Moodle page of the module. \n",
    "- 2. Then clone the Git repository from the GitHub server that GitHub will create for you. Initially it will contain README.md and a folder structure for Part One and Part Two with placeholder python scripts (you can change these to Jupyter notebook files if you prefer). \n",
    "- 3. Enter your name in README.md (this makes it easy for us to see whose code we are marking) \n",
    "- 4. You must also enter the following Academic Declaration into README.md for your submission:\n",
    "> ‚ÄúI have read and understood the sections of plagiarism in the College Policy on assessment offences and confirm that the work is my own, with the work of others clearly acknowledged. I give my permission to submit my report to the plagiarism testing database that the College is using and test it using plagiarism detection software, search engines or meta-searching software.‚Äù \n",
    "This refers to the document at: [link](https://www.bbk.ac.uk/student-services/exams/plagiarism-guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66073a",
   "metadata": {},
   "source": [
    "5. Whenever you have made a change that can ‚Äústand on its own‚Äù, say, ‚ÄúImplemented tokenizer method‚Äù, this is a good opportunity to commit the change to your local repository and also to push your changed local repository to the GitHub server. As a rule of thumb, in collaborative software development it is common to require that the code base should at least still compile after each commit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858b878",
   "metadata": {},
   "source": [
    "- Entering your name in README.md (using a text editor), then doing a commit of your change to the file into the local repository, and finally doing a push of your local repository to the GitHub server would be an excellent way to start your coursework activities. \n",
    "- You can benefit from the GitHub server also to synchronise between, e.g., the Birkbeck lab machines and your own computer. You push the state of your local repository in the lab to the GitHub server before you go home; later, you can pull your changes to the repository on your home computer (and vice versa). Use meaningful commit messages (e.g., ‚ÄúImplemented the pickle output for Q1(e)‚Äù, or ‚Äúfixed a bug in the PMI calculation‚Äù), and do not forget to push your changes to the GitHub server! \n",
    "- For marking, we plan to clone your repositories from the GitHub server shortly after the submission deadline. We additionally require you to confirm your github submission via a ‚Äòconfirm submission‚Äô form on Moodle. The time of this confirmation will tell us if you would like your code to be considered for the regular (uncapped) deadline or for the late (capped) deadline two weeks later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37871d1",
   "metadata": {},
   "source": [
    "Deadlines \n",
    "-  The submission deadline is: 26th June 2025, 14:00 UK time. \n",
    "- The late cut-off deadline for receiving a capped mark is: 10th July 2025, 14:00 UK time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
