{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e66577",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Part One — Syntax and Style \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a6494",
   "metadata": {},
   "source": [
    "In the first part of your coursework, your task is to explore the syntax and style of a set of 19th Century novels using the methods and tools that you learned in class. The texts you need for this part are in the novels subdirectory of the texts directory in the coursework Moodle template. The texts are in plain text files, and the filenames include the title, author, and year of publication, separated by hyphens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5280ae",
   "metadata": {},
   "source": [
    "The template code provided in PartOne.py includes function headers for some sub-parts of this ques- tion. The main method of your finished script should call each of these functions in order. To complete your coursework, complete these functions so that they perform the tasks specified in the questions below. You may (and in some cases should) define additional functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269732d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Re-assessment template 2025\n",
    "\n",
    "Note: The template functions here and the dataframe format for structuring your solution is a suggested but not mandatory approach. You can use a different approach if you like, as long as you clearly answer the questions and communicate your answers clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ed11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# import nltk.word_tokenize # incorrect -- raises error see: https://www.nltk.org/api/nltk.tokenize.html\n",
    "# import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f579f61",
   "metadata": {},
   "source": [
    "## read_novels:\n",
    "Each file in the novels directory contains the text of a novel, and the name of the file is the title, author, and year of publication of the novel, separated by hyphens. Complete the python function read_texts to do the following: \n",
    "- i. create a pandas dataframe with the following columns: text, title, author, year \n",
    "- ii. sort the dataframe by the year column before returning it, resetting or ignoring the dataframe index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_novels(path=Path.cwd() / \"zips\" / \"p1-texts\" / \"novels\"):\n",
    "    \"\"\"Reads texts from a directory of .txt files and returns a DataFrame with the text, title,\n",
    "    author, and year\"\"\"\n",
    "    files = [file for file in os.listdir(path) if file[-4:] == '.txt']\n",
    "    df_data = []\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        ''' Create a dictionary, and add the file items to it, then use populated dictionary to create a dataframe '''\n",
    "        file_dict = {}\n",
    "        file_dict['text'] = read_file(os.path.join(path,files[i]))\n",
    "        title, author, year = files[i].split('-')\n",
    "        file_dict['title'] = title \n",
    "        file_dict['author'] = author\n",
    "        file_dict['year'] = year[:-4]\n",
    "        df_data.append(file_dict)\n",
    "    df = pd.DataFrame(df_data)     \n",
    "    df = df.sort_values('year') # return sorted by year (assumed default ascending)\n",
    "    df.reset_index(inplace=True, drop=True)     \n",
    "    return df     \n",
    "    \n",
    "df = read_novels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8dda8",
   "metadata": {},
   "source": [
    "## nltk_ttr: \n",
    "This function should return a dictionary mapping the title of each novel to its type-token ratio. Tokenize the text using the NLTK library only. Do not include punctuation as tokens, and ignore case when counting types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_ttr(text):\n",
    "    \"\"\"Calculates the type-token ratio of a text. Text is tokenized using nltk.word_tokenize.\"\"\"\n",
    "    text_cleaned = [t.lower() for t in word_tokenize(text) if t.isalnum()] # I've assumed that we want the numbers, as well as text.\n",
    "    vocab = set(text_cleaned) # dedupe words\n",
    "    ttr = len(vocab) / len(text_cleaned) # ratio words to the text\n",
    "    return ttr\n",
    "\n",
    "\n",
    "def get_ttrs(df):\n",
    "    \"\"\"helper function to add ttr to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = nltk_ttr(row[\"text\"])\n",
    "    return results\n",
    "get_ttrs(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223eb32d",
   "metadata": {},
   "source": [
    "## c) flesch_kincaid: WIP\n",
    "This function should return a dictionary mapping the title of each novel to the Flesch-Kincaid reading grade level score of the text.  \n",
    "Use the NLTK library for tokenization and the CMU pronouncing dictionary for estimating syllable counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b11b2",
   "metadata": {},
   "source": [
    "[Readibility](https://readabilityformulas.com/learn-how-to-use-the-flesch-kincaid-grade-level/)\n",
    "$$\n",
    "{\\displaystyle 0.39 * \\left({\\frac {\\text{total words}}{\\text{total sentences}}}\\right)\n",
    "    +11.8\\left({\\frac {\\text{total syllables}}{\\text{total words}}}\\right)-15.59 }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601f0f7",
   "metadata": {},
   "source": [
    "Work in progress FL-GL function.\n",
    "Currently only counts a word once when calculating syllables - but this needs to be total syllables (allow duplicates).\n",
    "Next - use temp dictionary, and each word get count.\n",
    "temp dict value * word count = total syllables for that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097f9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_syl(word, d):\n",
    "    import re\n",
    "    \"\"\"Counts the number of syllables in a word given a dictionary of syllables per word.\n",
    "    if the word is not in the dictionary, syllables are estimated by counting vowel clusters\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to count syllables for.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of syllables in the word.\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r'[aeiouy]+'\n",
    "\n",
    "    vowel_words = re.findall(pattern, text.lower())\n",
    "    n_vowel_clusters = len(vowel_words)\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4017d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk \n",
    "def fk_level(text, d):\n",
    "    \"\"\"Returns the Flesch-Kincaid Grade Level of a text (higher grade is more difficult).\n",
    "    Requires a dictionary of syllables per word.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        float: The Flesch-Kincaid Grade Level of the text. (higher grade is more difficult)\n",
    "    \"\"\"\n",
    "    sents=sent_tokenize(text.lower())\n",
    "    words=[t.lower() for t in word_tokenize(text) if t.isalpha()]\n",
    "    temp = dict()\n",
    "        \n",
    "    for word in words:\n",
    "        cmu_counter = 0\n",
    "        if word in d:\n",
    "            temp[word] = None\n",
    "            for values in d[word]:\n",
    "                counts = 0 # return the maximum value\n",
    "                for value in values:\n",
    "                    v = sum(char[-1].isdigit() for char in value)\n",
    "                    counts += v\n",
    "                    '''  WIP  - not quite right - dictionary counts each word once...\n",
    "                    if temp[word] == None:\n",
    "                        temp[word]= counts\n",
    "                    else:\n",
    "                        if counts > temp[word]:\n",
    "                            temp[word]= counts\n",
    "                    '''\n",
    "    total_syllables = sum(temp.values()) #! this isn't quite right - it has in effect deduplicated the words whereas the other sums do not\n",
    "    fkl = 0.39 * (len(words) / len(sents)) + 11.8 * (total_syllables/ len(words))- 15.59\n",
    "    print(total_syllables, len(words), len(sents))\n",
    "\n",
    "    return fkl\n",
    "\n",
    "\n",
    "def get_fks(df):\n",
    "    \"\"\"helper function to add fk scores to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    cmudict = nltk.corpus.cmudict.dict()\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = round(fk_level(row[\"text\"], cmudict), 4)\n",
    "    return results\n",
    "\n",
    "\n",
    "get_fks(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762e657",
   "metadata": {},
   "source": [
    "## d) flesch_kincaid **todo**\n",
    "When is the Flesch Kincaid score *not* a valid, robust or reliable estimator of text difficulty? Give two conditions. (Text answer, 200 words maximum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f6997",
   "metadata": {},
   "source": [
    "## e) parse: WIP\n",
    "The goal of this function is to process the texts with spaCy’s tokenizer and parser, and store the processed texts. Your completed function should: \n",
    "- i. Use the spaCy nlp method to add a new column to the dataframe that contains parsed and tokenized Doc objects for each text. \n",
    "- ii. Serialise the resulting dataframe (i.e., write it out to disk) using the pickle format. \n",
    "- iii. Return the dataframe. \n",
    "- iv. Load the dataframe from the pickle file and use it for the remainder of this coursework part. Note: one or more of the texts may exceed the default maximum length for spaCy’s model. You will need to either increase this length or parse the text in sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad318b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse(df, store_path=Path.cwd() / \"pickles\", out_name=\"parsed.pickle\"):\n",
    "    \"\"\"Parses the text of a DataFrame using spaCy, stores the parsed docs as a column and writes the resulting  DataFrame to a pickle file\"\"\"\n",
    "    df['docs'] = [nlp(line) for line in df['text']] # create spacy docs for each cell \"text\"\n",
    "    df.to_pickle(os.path.join(store_path,out_name)) # write to pickle\n",
    "    return df\n",
    "    \n",
    "def load_from_pickle(store_path=Path.cwd() / \"pickles\", out_name=\"parsed.pickle\"):\n",
    "    df = pd.read_pickle(os.path.join(store_path, out_name))\n",
    "    return df\n",
    "\n",
    "#! NOT TESTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208444b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b56f494c",
   "metadata": {},
   "source": [
    "## f) Working with parses: **todo**\n",
    "the final lines of the code template contain three `for` loops. Write the functions needed to complete these loops so that they print: \n",
    "- i. The title of each novel and a list of the ten most common syntactic objects overall in the text. \n",
    "- ii. The title of each novel and a list of the ten most common syntactic subjects of the verb ‘to hear’ (in any tense) in the text, ordered by their frequency. \n",
    "- iii. The title of each novel and a list of the ten most common syntactic subjects of the verb ‘to hear’ (in any tense) in the text, ordered by their Pointwise Mutual Information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjective_counts(doc):\n",
    "    \"\"\"Extracts the most common adjectives in a parsed document. Returns a list of tuples.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def subjects_by_verb_pmi(doc, target_verb):\n",
    "    \"\"\"Extracts the most common subjects of a given verb in a parsed document. Returns a list.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def subjects_by_verb_count(doc, verb):\n",
    "    \"\"\"Extracts the most common subjects of a given verb in a parsed document. Returns a list.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba90f4",
   "metadata": {},
   "source": [
    "## g) Ten marks are allocated for your github commit history. \n",
    "You should make regular, atomic commits with concise but informative commit messages. See the section titled Submission (both questions) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    uncomment the following lines to run the functions once you have completed them\n",
    "    \"\"\"\n",
    "    #path = Path.cwd() / \"p1-texts\" / \"novels\"\n",
    "    #print(path)\n",
    "    #df = read_novels(path) # this line will fail until you have completed the read_novels function above.\n",
    "    #print(df.head())\n",
    "    #nltk.download(\"cmudict\")\n",
    "    #parse(df)\n",
    "    #print(df.head())\n",
    "    #print(get_ttrs(df))\n",
    "    #print(get_fks(df))\n",
    "    #df = pd.read_pickle(Path.cwd() / \"pickles\" /\"name.pickle\")\n",
    "    # print(adjective_counts(df))\n",
    "    \"\"\" \n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_count(row[\"parsed\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_pmi(row[\"parsed\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e32ce",
   "metadata": {},
   "source": [
    "# Part Two — Feature Extraction and Classification \n",
    "In the second part of the coursework, your task is to train and test machine learning classifiers on a dataset of political speeches. The objective is to learn to predict the political party from the text of the speech. The texts you need for this part are in the speeches sub-directory of the texts directory of the coursework Moodle template. For this part, you can structure your python functions in any way that you like, but pay attention to exactly what information (if any) you are asked to print out in each part. Your final script should print out the answers to each part where required, and nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0751f3",
   "metadata": {},
   "source": [
    "## a) Read the hansard40000.csv dataset in the texts directory into a dataframe. \n",
    "Sub- set and rename the dataframe as follows: \n",
    "- i. rename the ‘Labour (Co-op)’ value in ‘party’ column to ‘Labour’, and then: \n",
    "- ii. remove any rows where the value of the ‘party’ column is not one of the four most common party names, and remove the ‘Speaker’ value. \n",
    "- iii. remove any rows where the value in the ‘speech_class’ column is not ‘Speech’. \n",
    "- iv. remove any rows where the text in the ‘speech’ column is less than 1000 characters long\n",
    "- Print the dimensions of the resulting dataframe using the shape method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_df(path=Path.cwd() / \"zips\" / \"p2-texts\"):\n",
    "    df = pd.read_csv(os.path.join(path,os.listdir(path)[0]))\n",
    "\n",
    "    #^ - i. rename the ‘Labour (Co-op)’ value in ‘party’ column to ‘Labour’, and then: \n",
    "    df['party'].replace(\"Labour (Co-op)\", \"Labour\", inplace=True)\n",
    "\n",
    "\n",
    "    #^ - ii. remove any rows where the value of the ‘party’ column is not one of the four most common party names, and remove the ‘Speaker’ value. \n",
    "    values = ['Conservative', 'Labour', 'Scottish National Party', 'Liberal Democrat',]\n",
    "    df = df[df['party'].isin(values)]\n",
    "\n",
    "    #^ - iii. remove any rows where the value in the ‘speech_class’ column is not ‘Speech’. \n",
    "    df = df[df['speech_class'] == 'Speech']\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True) #? NB: needs reseting, otherwise the next step doesn't work\n",
    "\n",
    "\n",
    "    #^ - iv. remove any rows where the text in the ‘speech’ column is less than 1000 characters long\n",
    "    # indices= []\n",
    "    ind= []\n",
    "    for i, s in enumerate(df['speech'].values):\n",
    "        # print(len(s))\n",
    "        if len(s) < 1000:\n",
    "            # indices.append((i, len(s)))\n",
    "            ind.append(i)\n",
    "\n",
    "    df = df[~df.index.isin(ind)]\n",
    "\n",
    "    #^ - Print the dimensions of the resulting dataframe using the shape method.\n",
    "    # len(df) == 36223-28139\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07271048",
   "metadata": {},
   "source": [
    "## b) Vectorise the speeches using TfidfVectorizer from scikit-learn. \n",
    "Use the default parameters, except for omitting English stopwords and setting max_features to 3000. Split the data into a train and test set, using stratified sampling, with a random seed of 26."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c2f73",
   "metadata": {},
   "source": [
    "## c) Random forest and SVM\n",
    "Train RandomForest (with n_estimators=300) and SVM with linear kernel classifiers on the training set, and print the scikit-learn macro-average f1 score and classification report for each classifier on the test set. The label that you are trying to predict is the ‘party’ value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885cf15",
   "metadata": {},
   "source": [
    "## d) N-Grams\n",
    "Adjust the parameters of the Tfidfvectorizer so that unigrams, bi-grams and tri-grams will be considered as features, limiting the total number of features to 3000. Print the classification report as in 2(c) again using these parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c9d2c",
   "metadata": {},
   "source": [
    "## e) Implement a new custom tokenizer and pass it to the tokenizer argument of Tfidfvectorizer. \n",
    "You can use this function in any way you like to try to achieve the best classification performance while keeping the number of features to no more than 3000, and using the same three classifiers as above. Print the clas- sification report for the best performing classifier using your tokenizer. Marks will be awarded both for a high overall classification performance, and a good trade-off between classification performance and efficiency (i.e., using fewer parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e82e04",
   "metadata": {},
   "source": [
    "## f) Explain your tokenizer function and discuss its performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf42e72",
   "metadata": {},
   "source": [
    "## g) Githistory\n",
    "Ten marks are allocated for your github commit history. You should make regular, atomic commits with concise but informative commit messages. See the section below titled Submission (both questions) for more details. Part Two total marks: 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86baa4d6",
   "metadata": {},
   "source": [
    "# Git requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c7cfc",
   "metadata": {},
   "source": [
    "Submit using github classroom and confirm on Moodle The code template for this coursework part is made available to you as a Git repository on GitHub, via an invitation link for GitHub Classroom. \n",
    "\n",
    "- 1. First you follow the invitation link for the coursework that is available on the Moodle page of the module. \n",
    "- 2. Then clone the Git repository from the GitHub server that GitHub will create for you. Initially it will contain README.md and a folder structure for Part One and Part Two with placeholder python scripts (you can change these to Jupyter notebook files if you prefer). \n",
    "- 3. Enter your name in README.md (this makes it easy for us to see whose code we are marking) \n",
    "- 4. You must also enter the following Academic Declaration into README.md for your submission:\n",
    "> “I have read and understood the sections of plagiarism in the College Policy on assessment offences and confirm that the work is my own, with the work of others clearly acknowledged. I give my permission to submit my report to the plagiarism testing database that the College is using and test it using plagiarism detection software, search engines or meta-searching software.” \n",
    "This refers to the document at: [link](https://www.bbk.ac.uk/student-services/exams/plagiarism-guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66073a",
   "metadata": {},
   "source": [
    "5. Whenever you have made a change that can “stand on its own”, say, “Implemented tokenizer method”, this is a good opportunity to commit the change to your local repository and also to push your changed local repository to the GitHub server. As a rule of thumb, in collaborative software development it is common to require that the code base should at least still compile after each commit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858b878",
   "metadata": {},
   "source": [
    "- Entering your name in README.md (using a text editor), then doing a commit of your change to the file into the local repository, and finally doing a push of your local repository to the GitHub server would be an excellent way to start your coursework activities. \n",
    "- You can benefit from the GitHub server also to synchronise between, e.g., the Birkbeck lab machines and your own computer. You push the state of your local repository in the lab to the GitHub server before you go home; later, you can pull your changes to the repository on your home computer (and vice versa). Use meaningful commit messages (e.g., “Implemented the pickle output for Q1(e)”, or “fixed a bug in the PMI calculation”), and do not forget to push your changes to the GitHub server! \n",
    "- For marking, we plan to clone your repositories from the GitHub server shortly after the submission deadline. We additionally require you to confirm your github submission via a ‘confirm submission’ form on Moodle. The time of this confirmation will tell us if you would like your code to be considered for the regular (uncapped) deadline or for the late (capped) deadline two weeks later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37871d1",
   "metadata": {},
   "source": [
    "Deadlines \n",
    "-  The submission deadline is: 26th June 2025, 14:00 UK time. \n",
    "- The late cut-off deadline for receiving a capped mark is: 10th July 2025, 14:00 UK time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94497467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be23076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8341da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cf6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80e2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
