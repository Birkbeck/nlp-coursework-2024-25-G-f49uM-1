{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e66577",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Part One — Syntax and Style \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a6494",
   "metadata": {},
   "source": [
    "In the first part of your coursework, your task is to explore the syntax and style of a set of 19th Century novels using the methods and tools that you learned in class. The texts you need for this part are in the novels subdirectory of the texts directory in the coursework Moodle template. The texts are in plain text files, and the filenames include the title, author, and year of publication, separated by hyphens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5280ae",
   "metadata": {},
   "source": [
    "The template code provided in PartOne.py includes function headers for some sub-parts of this ques- tion. The main method of your finished script should call each of these functions in order. To complete your coursework, complete these functions so that they perform the tasks specified in the questions below. You may (and in some cases should) define additional functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269732d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Re-assessment template 2025\n",
    "\n",
    "Note: The template functions here and the dataframe format for structuring your solution is a suggested but not mandatory approach. You can use a different approach if you like, as long as you clearly answer the questions and communicate your answers clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# import nltk.word_tokenize # incorrect -- raises error see: https://www.nltk.org/api/nltk.tokenize.html\n",
    "# import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f579f61",
   "metadata": {},
   "source": [
    "## read_novels:\n",
    "Each file in the novels directory contains the text of a novel, and the name of the file is the title, author, and year of publication of the novel, separated by hyphens. Complete the python function read_texts to do the following: \n",
    "- i. create a pandas dataframe with the following columns: text, title, author, year \n",
    "- ii. sort the dataframe by the year column before returning it, resetting or ignoring the dataframe index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def remove_chapter_headings(text):\n",
    "    pattern = r'(?i)CHAPTER \\d+'  #* adding this so that chapter breaks are not counted. could be refined for section breaks etc.\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "\n",
    "def read_novels(path=Path.cwd() / \"zips\" / \"p1-texts\" / \"novels\"):\n",
    "    \"\"\"Reads texts from a directory of .txt files and returns a DataFrame with the text, title,\n",
    "    author, and year\"\"\"\n",
    "    files = [file for file in os.listdir(path) if file[-4:] == '.txt']\n",
    "    df_data = []\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        ''' Create a dictionary, and add the file items to it, then use populated dictionary to create a dataframe '''\n",
    "        file_dict = {}\n",
    "        file_dict['text'] = read_file(os.path.join(path,files[i]))\n",
    "        title, author, year = files[i].split('-')\n",
    "        file_dict['title'] = title \n",
    "        file_dict['author'] = author\n",
    "        file_dict['year'] = year[:-4]\n",
    "        df_data.append(file_dict)\n",
    "    df = pd.DataFrame(df_data)\n",
    "    df['text_c'] = df['text'].copy() # retain original copy\n",
    "    df['text'] = df['text'].apply(lambda x: remove_chapter_headings(x).strip()) # apply function to all items in text column\n",
    "    df = df[['text_c','text', 'title', 'author', 'year']] # reorder columns\n",
    "    df = df.sort_values('year') # return sorted by year (assumed default ascending)\n",
    "    df.reset_index(inplace=True, drop=True)     \n",
    "    return df     \n",
    "    \n",
    "df = read_novels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1beddc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_c</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nCHAPTER 1\\n\\nThe family of Dashwood had long...</td>\n",
       "      <td>The family of Dashwood had long been settled i...</td>\n",
       "      <td>Sense_and_Sensibility</td>\n",
       "      <td>Austen</td>\n",
       "      <td>1811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Wooed and married and a'.'\\n'Edith!' said Mar...</td>\n",
       "      <td>'Wooed and married and a'.'\\n'Edith!' said Mar...</td>\n",
       "      <td>North_and_South</td>\n",
       "      <td>Gaskell</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_c  \\\n",
       "0  \\nCHAPTER 1\\n\\nThe family of Dashwood had long...   \n",
       "1  'Wooed and married and a'.'\\n'Edith!' said Mar...   \n",
       "\n",
       "                                                text                  title  \\\n",
       "0  The family of Dashwood had long been settled i...  Sense_and_Sensibility   \n",
       "1  'Wooed and married and a'.'\\n'Edith!' said Mar...        North_and_South   \n",
       "\n",
       "    author  year  \n",
       "0   Austen  1811  \n",
       "1  Gaskell  1855  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8dda8",
   "metadata": {},
   "source": [
    "## nltk_ttr: \n",
    "This function should return a dictionary mapping the title of each novel to its type-token ratio. Tokenize the text using the NLTK library only. Do not include punctuation as tokens, and ignore case when counting types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d77ee3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sense_and_Sensibility': 0.05288568542519132,\n",
       " 'North_and_South': 0.0549040694681204,\n",
       " 'A_Tale_of_Two_Cities': 0.07075401657114008,\n",
       " 'Erewhon': 0.0916875393290313,\n",
       " 'The_American': 0.06385674022547373,\n",
       " 'Dorian_Gray': 0.08359599310916864,\n",
       " 'Tess_of_the_DUrbervilles': 0.0778145359339165,\n",
       " 'The_Golden_Bowl': 0.04748457874040203,\n",
       " 'The_Secret_Garden': 0.05847231570812455,\n",
       " 'Portrait_of_the_Artist': 0.10497357039884671,\n",
       " 'The_Black_Moth': 0.07870315352786751,\n",
       " 'Orlando': 0.11390939170272334,\n",
       " 'Blood_Meridian': 0.08570454932697223}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nltk_ttr(text):\n",
    "    \"\"\"Calculates the type-token ratio of a text. Text is tokenized using nltk.word_tokenize.\"\"\"\n",
    "    text_cleaned = [t.lower() for t in word_tokenize(text) if t.isalnum()] # I've assumed that we want the numbers, as well as text.\n",
    "    vocab = set(text_cleaned) # dedupe words\n",
    "    ttr = len(vocab) / len(text_cleaned) # ratio words to the text\n",
    "    return ttr\n",
    "\n",
    "\n",
    "def get_ttrs(df):\n",
    "    \"\"\"helper function to add ttr to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = nltk_ttr(row[\"text\"])\n",
    "    return results\n",
    "get_ttrs(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223eb32d",
   "metadata": {},
   "source": [
    "## c) flesch_kincaid\n",
    "This function should return a dictionary mapping the title of each novel to the Flesch-Kincaid reading grade level score of the text.  \n",
    "Use the NLTK library for tokenization and the CMU pronouncing dictionary for estimating syllable counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b11b2",
   "metadata": {},
   "source": [
    "[Readibility](https://readabilityformulas.com/learn-how-to-use-the-flesch-kincaid-grade-level/)\n",
    "$$\n",
    "{\\displaystyle 0.39 * \\left({\\frac {\\text{total words}}{\\text{total sentences}}}\\right)\n",
    "    +11.8\\left({\\frac {\\text{total syllables}}{\\text{total words}}}\\right)-15.59 }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a51dde5",
   "metadata": {},
   "source": [
    "### `count_syl()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b097f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import cmudict\n",
    "def count_syl(word, d):\n",
    "    \"\"\"Counts the number of syllables in a word given a dictionary of syllables per word.\n",
    "    if the word is not in the dictionary, syllables are estimated by counting vowel clusters\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to count syllables for.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of syllables in the word.\n",
    "    \"\"\"\n",
    "    counts = 0\n",
    "    pattern = r'[aeiouy]+'\n",
    "\n",
    "    if word in d:\n",
    "        word_cmu = d[word][0]\n",
    "        counts += sum(char[-1].isdigit() for char in word_cmu)\n",
    "    elif word not in d:\n",
    "        vowel_words = re.findall(pattern, word.lower())\n",
    "        counts += len(vowel_words)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc007655",
   "metadata": {},
   "source": [
    "##### Test for `count_syl()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaacaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[('the', 1), ('family', 3), ('of', 1), ('dashwood', 2), ('had', 1), ('long', 1), ('been', 1), ('settled', 2), ('in', 1), ('sussex', 2), ('aaaa', 1), ('maaattaan', 2)]\n"
     ]
    }
   ],
   "source": [
    "# text = df['text'][0]\n",
    "# pattern = r'chapter [0-9] +'\n",
    "# text = text.replace('\\n', ' ').strip()\n",
    "# text = re.sub(pattern, '', text.lower())\n",
    "# d = cmudict.dict()\n",
    "# sents=sent_tokenize(text.lower())\n",
    "# words=[t.lower() for t in word_tokenize(text) if t.isalpha()]\n",
    "\n",
    "# x = words[0:10] # sample of the words\n",
    "\n",
    "# ''' Manual text of the function vs local use of the cmu dictionary '''\n",
    "\n",
    "# temp=dict()\n",
    "# for i in x:\n",
    "#     counts=0\n",
    "#     word_cmu = d[i][0]\n",
    "#     counts += sum(char[-1].isdigit() for char in word_cmu)\n",
    "#     temp[i] = (counts, word_cmu)\n",
    "# print([t[0] for t in temp.values()] == [count_syl(i, d) for i in x])\n",
    "\n",
    "# ''' And a test with some words not in the cmu dictionary'''\n",
    "# y = ['aaaa', 'maaattaan']\n",
    "# z = x+y\n",
    "# print(list(zip(z,[count_syl(i, d) for i in z])))\n",
    "# print(sum([count_syl(word, d) for word in z]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e405d",
   "metadata": {},
   "source": [
    "### `fk_level()`, `get_fks()`, `add_fks_to_df()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4017d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk \n",
    "import re\n",
    "def fk_level(text, d):\n",
    "    \"\"\"Returns the Flesch-Kincaid Grade Level of a text (higher grade is more difficult).\n",
    "    Requires a dictionary of syllables per word.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        float: The Flesch-Kincaid Grade Level of the text. (higher grade is more difficult)\n",
    "    \"\"\"\n",
    "    text = text.replace('\\n', ' ').strip() #* And removing line breaks \"\\n\"\n",
    "    sents=sent_tokenize(text.lower())\n",
    "    words=[t.lower() for t in word_tokenize(text) if t.isalpha()]\n",
    "    total_syllables = sum([count_syl(word, d) for word in words])\n",
    "    fkl = 0.39 * (len(words) / len(sents)) + 11.8 * (total_syllables/ len(words))- 15.59\n",
    "    # print(total_syllables, len(words), len(sents), fkl)\n",
    "\n",
    "    return fkl\n",
    "\n",
    "\n",
    "def get_fks(df):\n",
    "    \"\"\"helper function to add fk scores to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    cmudict = nltk.corpus.cmudict.dict()\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = round(fk_level(row[\"text\"], cmudict), 4)\n",
    "    return results\n",
    "\n",
    "def add_fks_to_df(df):\n",
    "    ''' function to add scores to the dataframe, as referenced in get_fks() '''\n",
    "    fks = get_fks(df)\n",
    "    temp = pd.DataFrame({'title_f': fks.keys(), 'fks':fks.values()})\n",
    "    df = df.merge(temp, left_on='title', right_on='title_f')\n",
    "    return df.drop(columns='title_f')\n",
    "df = add_fks_to_df(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762e657",
   "metadata": {},
   "source": [
    "## d) flesch_kincaid **todo**\n",
    "When is the Flesch Kincaid score *not* a valid, robust or reliable estimator of text difficulty? Give two conditions. (Text answer, 200 words maximum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f6997",
   "metadata": {},
   "source": [
    "## e) parse\n",
    "The goal of this function is to process the texts with spaCy’s tokenizer and parser, and store the processed texts. Your completed function should: \n",
    "- i. Use the spaCy nlp method to add a new column to the dataframe that contains parsed and tokenized Doc objects for each text. \n",
    "- ii. Serialise the resulting dataframe (i.e., write it out to disk) using the pickle format. \n",
    "- iii. Return the dataframe. \n",
    "- iv. Load the dataframe from the pickle file and use it for the remainder of this coursework part. Note: one or more of the texts may exceed the default maximum length for spaCy’s model. You will need to either increase this length or parse the text in sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad318b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse(df, store_path=Path.cwd() / \"pickles\", out_name=\"parsed.pickle\"):\n",
    "    \"\"\"Parses the text of a DataFrame using spaCy, stores the parsed docs as a column and writes the resulting  DataFrame to a pickle file\"\"\"\n",
    "    docs = {}\n",
    "    for i in range(len(df)):\n",
    "        docs[i] = nlp(df['text'][i]) # create spacy docs for each cell \"text\"\n",
    "        # print(f\"Document {i+1} / {len(df)} parsed.\") #? added to see process when parsing (takes a while)\n",
    "    df['docs'] = docs.values()\n",
    "    df.to_pickle(os.path.join(store_path,out_name)) #~ write to pickle\n",
    "    # return df #^ To check the returned df\n",
    "    \n",
    "def load_from_pickle(store_path=Path.cwd() / \"pickles\", out_name=\"parsed.pickle\"):\n",
    "    df = pd.read_pickle(os.path.join(store_path, out_name))\n",
    "    return df\n",
    "\n",
    "# df = parse(df) #^ To check the returned df with spacy doc objects\n",
    "\n",
    "''' Load from pickle '''\n",
    "df = load_from_pickle() \n",
    "\n",
    "#? To check the returned type\n",
    "# type(df['docs'][0]) # ~spacy.tokens.doc.Doc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f494c",
   "metadata": {},
   "source": [
    "## f) Working with parses: **todo**\n",
    "the final lines of the code template contain three `for` loops. Write the functions needed to complete these loops so that they print: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19bb02",
   "metadata": {},
   "source": [
    "### i. The title of each novel and a list of the ten most common syntactic objects overall in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def find_objects(token):\n",
    "    ''' Function to find the objects of a token  '''\n",
    "    objects = set()\n",
    "    if 'obj' in token.dep_:\n",
    "        objects.add(token)\n",
    "    return objects\n",
    "\n",
    "\n",
    "\n",
    "def objects_count(doc, n=10):\n",
    "    object_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        objects = find_objects(token)\n",
    "        if objects:\n",
    "            object_text.extend([objects.text.lower() for objects in objects]) #? have made this lower, to not duplicate the types\n",
    "\n",
    "    object_counts = Counter(object_text)\n",
    "    return  object_counts.most_common()[:n]\n",
    "# doc = nlp(\"We have never finished Hamlet, Marianne; our dear Willoughby went away before we could get through it. Before the middle of the day, they were visited by Sir John and Mrs. Jennings, who, having heard of the arrival of a gentleman at the cottage, came to take a survey of the guest.\")\n",
    "objects_count(df['docs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjective_counts(doc, n=10):\n",
    "    \"\"\"Extracts the most common adjectives in a parsed document. Returns a list of tuples.\"\"\"\n",
    "    pos_counts = Counter([token.text for token in doc if token.pos_ == 'ADJ'])\n",
    "    return pos_counts.most_common()[:n]\n",
    "    \n",
    "# doc = df['docs'][len(df)-1] #*to test function with a sample doc\n",
    "# adjective_counts(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f8de6",
   "metadata": {},
   "source": [
    "### ii. The title of each novel and a list of the ten most common syntactic subjects of the verb ‘to hear’ (in any tense) in the text, ordered by their frequency. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905dd967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 21),\n",
       " ('he', 19),\n",
       " ('you', 12),\n",
       " ('she', 10),\n",
       " ('they', 5),\n",
       " ('monseigneur', 2),\n",
       " ('me', 2),\n",
       " ('one', 1),\n",
       " ('jerry', 1),\n",
       " ('stranger', 1),\n",
       " ('clink', 1),\n",
       " ('echoes', 1),\n",
       " ('we', 1),\n",
       " ('her', 1),\n",
       " ('mother', 1),\n",
       " ('lucie', 1),\n",
       " ('jar', 1),\n",
       " ('him', 1),\n",
       " ('darnay', 1),\n",
       " ('d’ye', 1),\n",
       " ('voice', 1),\n",
       " ('sound', 1),\n",
       " ('his', 1),\n",
       " ('matter', 1),\n",
       " ('pross', 1)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_subjects(token):\n",
    "    ''' Function to find the subjects of a token - both up and down the parse tree '''\n",
    "    subjects = []\n",
    "    ''' Token's children '''\n",
    "    for child in token.children:\n",
    "        if 'subj' in child.dep_:\n",
    "            subjects.append(child)\n",
    "    return subjects\n",
    "\n",
    "def subjects_by_verb_count(doc, verb, trim_singles=True):\n",
    "    \"\"\"Extracts the most common subjects of a given verb in a parsed document. Returns a list.\"\"\"\n",
    "    ''' Added a parameter to remove multiple counts of 1 in the most frequent.\n",
    "    > Setting to n=10 sliced the list by order, which didn't have much meaning in the returned values\n",
    "    > Have opted to include the count\n",
    "    '''\n",
    "    subject_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\" and token.lemma_ == verb:\n",
    "            subjects = find_subjects(token)\n",
    "            if subjects:\n",
    "                subject_text.extend([subject.text.lower() for subject in subjects])\n",
    "    subject_counts = Counter(subject_text)\n",
    "    subject_counts = subject_counts.most_common()\n",
    "    if trim_singles:\n",
    "        return [x for x in subject_counts if x[1] >1]\n",
    "    else:\n",
    "        return  subject_counts\n",
    "\n",
    "\n",
    "\n",
    "# doc = df['docs'][2]\n",
    "# # doc=nlp(\"To hear is vital for him\")\n",
    "# # subjects_by_verb_count(doc, 'hear')\n",
    "# subjects_by_verb_count(doc,'hear', trim_singles=False)\n",
    "\n",
    "# for i, row in df.iterrows():\n",
    "#     print(row[\"title\"])\n",
    "#     print(subjects_by_verb_count(row[\"docs\"], \"hear\"))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77dbeb",
   "metadata": {},
   "source": [
    "### - iii. The title of each novel and a list of the ten most common syntactic subjects of the verb ‘to hear’ (in any tense) in the text, ordered by their Pointwise Mutual Information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7526e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def subjects_by_verb_pmi(doc, target_verb):\n",
    "    \"\"\"Extracts the most common subjects of a given verb in a parsed document. Returns a list.\"\"\"\n",
    "    pass\n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_pmi(row[\"parsed\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba90f4",
   "metadata": {},
   "source": [
    "## g) Ten marks are allocated for your github commit history. \n",
    "You should make regular, atomic commits with concise but informative commit messages. See the section titled Submission (both questions) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    uncomment the following lines to run the functions once you have completed them\n",
    "    \"\"\"\n",
    "    #path = Path.cwd() / \"p1-texts\" / \"novels\"\n",
    "    #print(path)\n",
    "    #df = read_novels(path) # this line will fail until you have completed the read_novels function above.\n",
    "    #print(df.head())\n",
    "    #nltk.download(\"cmudict\")\n",
    "    #parse(df)\n",
    "    #print(df.head())\n",
    "    #print(get_ttrs(df))\n",
    "    #print(get_fks(df))\n",
    "    #df = pd.read_pickle(Path.cwd() / \"pickles\" /\"name.pickle\")\n",
    "    # print(adjective_counts(df)) #? this is using the dataframe, the function and docstring calls for a `doc` object.\n",
    "    \"\"\" \n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_count(row[\"parsed\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_pmi(row[\"parsed\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e32ce",
   "metadata": {},
   "source": [
    "# Part Two — Feature Extraction and Classification \n",
    "In the second part of the coursework, your task is to train and test machine learning classifiers on a dataset of political speeches. The objective is to learn to predict the political party from the text of the speech. The texts you need for this part are in the speeches sub-directory of the texts directory of the coursework Moodle template. For this part, you can structure your python functions in any way that you like, but pay attention to exactly what information (if any) you are asked to print out in each part. Your final script should print out the answers to each part where required, and nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0751f3",
   "metadata": {},
   "source": [
    "## a) Read the hansard40000.csv dataset in the texts directory into a dataframe. \n",
    "Sub- set and rename the dataframe as follows: \n",
    "- i. rename the ‘Labour (Co-op)’ value in ‘party’ column to ‘Labour’, and then: \n",
    "- ii. remove any rows where the value of the ‘party’ column is not one of the four most common party names, and remove the ‘Speaker’ value. \n",
    "- iii. remove any rows where the value in the ‘speech_class’ column is not ‘Speech’. \n",
    "- iv. remove any rows where the text in the ‘speech’ column is less than 1000 characters long\n",
    "- Print the dimensions of the resulting dataframe using the shape method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03c8fc",
   "metadata": {},
   "source": [
    "Assuming we ignore blanks (`NaN`)\n",
    "\n",
    "`df['speech_class'].value_counts(dropna=False)`\n",
    "```\n",
    "party\n",
    "Conservative                        25079\n",
    "Labour                               8038\n",
    "Scottish National Party              2303\n",
    "NaN                                  1647\n",
    ".....\n",
    "\n",
    "`df['speech_class'].value_counts()`\n",
    "The top 4n less \"Speaker\"\n",
    "party\n",
    "Conservative                        25079\n",
    "Labour                               8038\n",
    "Scottish National Party              2303\n",
    "Liberal Democrat                      803\n",
    ".....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_df(path=Path.cwd() / \"zips\" / \"p2-texts\"):\n",
    "    df = pd.read_csv(os.path.join(path,os.listdir(path)[0]))\n",
    "\n",
    "    #^ - i. rename the ‘Labour (Co-op)’ value in ‘party’ column to ‘Labour’, and then: \n",
    "    df['party'].replace(\"Labour (Co-op)\", \"Labour\", inplace=True)\n",
    "\n",
    "\n",
    "    #^ - ii. remove any rows where the value of the ‘party’ column is not one of the four most common party names, and remove the ‘Speaker’ value. \n",
    "    values = ['Conservative', 'Labour', 'Scottish National Party', 'Liberal Democrat',]\n",
    "    df = df[df['party'].isin(values)]\n",
    "\n",
    "    #^ - iii. remove any rows where the value in the ‘speech_class’ column is not ‘Speech’. \n",
    "    df = df[df['speech_class'] == 'Speech']\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True) #? NB: needs reseting, otherwise the next step doesn't work\n",
    "\n",
    "\n",
    "    #^ - iv. remove any rows where the text in the ‘speech’ column is less than 1000 characters long\n",
    "    # indices= []\n",
    "    ind= []\n",
    "    for i, s in enumerate(df['speech'].values):\n",
    "        # print(len(s))\n",
    "        if len(s) < 1000:\n",
    "            # indices.append((i, len(s)))\n",
    "            ind.append(i)\n",
    "\n",
    "    df = df[~df.index.isin(ind)]\n",
    "\n",
    "    #^ - Print the dimensions of the resulting dataframe using the shape method.\n",
    "    # len(df) == 36223-28139\n",
    "    print(df.shape)\n",
    "    return df #? Although this might not be wanted? \"Function to print and nothing else?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = csv_to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07271048",
   "metadata": {},
   "source": [
    "## b) Vectorise the speeches using TfidfVectorizer from scikit-learn. \n",
    "Use the default parameters, except for omitting English stopwords and setting max_features to 3000. Split the data into a train and test set, using stratified sampling, with a random seed of 26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "random_seed = 26\n",
    "max_features = 3000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",max_features=max_features)\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.speech) \n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, df.party, random_state=random_seed, shuffle=True, stratify=df.party)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c2f73",
   "metadata": {},
   "source": [
    "## c) Random forest and SVM\n",
    "Train RandomForest (with n_estimators=300) and SVM with linear kernel classifiers on the training set, and print the scikit-learn macro-average f1 score and classification report for each classifier on the test set. The label that you are trying to predict is the ‘party’ value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score,  classification_report\n",
    "from sklearn import svm\n",
    "rfc = RandomForestClassifier(random_state=random_seed)\n",
    "n_estimators = 300\n",
    "\n",
    "rfc_fitted = rfc.fit(X_train, y_train)\n",
    "\n",
    "print(f1_score(y_test, rfc_fitted.predict(X_test), average='macro'))\n",
    "# params (y_true, y_predicted)\n",
    "print(classification_report(y_test, rfc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ee15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = svm.SVC(kernel='linear')\n",
    "linear_svc_fitted = linear_svc.fit(X_train, y_train)\n",
    "linear_svc_fitted.predict(X_test)\n",
    "\n",
    "print(f1_score(y_test, linear_svc.predict(X_test), average='macro'))\n",
    "print(classification_report(y_test, linear_svc_fitted.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885cf15",
   "metadata": {},
   "source": [
    "## d) N-Grams\n",
    "Adjust the parameters of the Tfidfvectorizer so that unigrams, bi-grams and tri-grams will be considered as features, limiting the total number of features to 3000. Print the classification report as in 2(c) again using these parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c9d2c",
   "metadata": {},
   "source": [
    "## e) Implement a new custom tokenizer and pass it to the tokenizer argument of Tfidfvectorizer. \n",
    "You can use this function in any way you like to try to achieve the best classification performance while keeping the number of features to no more than 3000, and using the same three classifiers as above. Print the clas- sification report for the best performing classifier using your tokenizer. Marks will be awarded both for a high overall classification performance, and a good trade-off between classification performance and efficiency (i.e., using fewer parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e82e04",
   "metadata": {},
   "source": [
    "## f) Explain your tokenizer function and discuss its performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf42e72",
   "metadata": {},
   "source": [
    "## g) Githistory\n",
    "Ten marks are allocated for your github commit history. You should make regular, atomic commits with concise but informative commit messages. See the section below titled Submission (both questions) for more details. Part Two total marks: 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86baa4d6",
   "metadata": {},
   "source": [
    "# Git requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c7cfc",
   "metadata": {},
   "source": [
    "Submit using github classroom and confirm on Moodle The code template for this coursework part is made available to you as a Git repository on GitHub, via an invitation link for GitHub Classroom. \n",
    "\n",
    "- 1. First you follow the invitation link for the coursework that is available on the Moodle page of the module. \n",
    "- 2. Then clone the Git repository from the GitHub server that GitHub will create for you. Initially it will contain README.md and a folder structure for Part One and Part Two with placeholder python scripts (you can change these to Jupyter notebook files if you prefer). \n",
    "- 3. Enter your name in README.md (this makes it easy for us to see whose code we are marking) \n",
    "- 4. You must also enter the following Academic Declaration into README.md for your submission:\n",
    "> “I have read and understood the sections of plagiarism in the College Policy on assessment offences and confirm that the work is my own, with the work of others clearly acknowledged. I give my permission to submit my report to the plagiarism testing database that the College is using and test it using plagiarism detection software, search engines or meta-searching software.” \n",
    "This refers to the document at: [link](https://www.bbk.ac.uk/student-services/exams/plagiarism-guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66073a",
   "metadata": {},
   "source": [
    "5. Whenever you have made a change that can “stand on its own”, say, “Implemented tokenizer method”, this is a good opportunity to commit the change to your local repository and also to push your changed local repository to the GitHub server. As a rule of thumb, in collaborative software development it is common to require that the code base should at least still compile after each commit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858b878",
   "metadata": {},
   "source": [
    "- Entering your name in README.md (using a text editor), then doing a commit of your change to the file into the local repository, and finally doing a push of your local repository to the GitHub server would be an excellent way to start your coursework activities. \n",
    "- You can benefit from the GitHub server also to synchronise between, e.g., the Birkbeck lab machines and your own computer. You push the state of your local repository in the lab to the GitHub server before you go home; later, you can pull your changes to the repository on your home computer (and vice versa). Use meaningful commit messages (e.g., “Implemented the pickle output for Q1(e)”, or “fixed a bug in the PMI calculation”), and do not forget to push your changes to the GitHub server! \n",
    "- For marking, we plan to clone your repositories from the GitHub server shortly after the submission deadline. We additionally require you to confirm your github submission via a ‘confirm submission’ form on Moodle. The time of this confirmation will tell us if you would like your code to be considered for the regular (uncapped) deadline or for the late (capped) deadline two weeks later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37871d1",
   "metadata": {},
   "source": [
    "Deadlines \n",
    "-  The submission deadline is: 26th June 2025, 14:00 UK time. \n",
    "- The late cut-off deadline for receiving a capped mark is: 10th July 2025, 14:00 UK time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94497467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be23076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8341da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cf6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80e2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
